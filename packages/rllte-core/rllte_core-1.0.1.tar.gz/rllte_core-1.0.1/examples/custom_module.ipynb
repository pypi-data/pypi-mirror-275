{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45f6a9b-3f46-419d-a081-77aaba26c0de",
   "metadata": {},
   "source": [
    "**RLLTE** is an extremely open platform that supports custom modules, including `encoder`, `storage`, `policy`, etc. Just write a new module based on the `BaseClass`, then we can insert it into an agent directly. Suppose we want to build a new encoder entitled `CustomEncoder`. An example is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbf596a-9ba4-487d-96e1-4fc5dbca5897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Invoking RLLTE Engine...\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - ================================================================================\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Tag               : ppo_atari\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Device            : NVIDIA GeForce RTX 3090\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Agent             : PPO\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Encoder           : CustomEncoder\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Policy            : OnPolicySharedActorCritic\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Storage           : VanillaRolloutStorage\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Distribution      : Categorical\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Augmentation      : False\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Intrinsic Reward  : False\n",
      "[08/29/2023 12:16:55 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - ================================================================================\n",
      "[08/29/2023 12:16:58 PM] - [\u001b[1m\u001b[32mEVAL.\u001b[0m] - S: 0           | E: 0           | L: 41          | R: 70.833      | T: 0:00:03    \n",
      "[08/29/2023 12:16:59 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 1024        | E: 8           | L: 52          | R: 123.000     | FPS: 216.774   | T: 0:00:04    \n",
      "[08/29/2023 12:17:00 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 2048        | E: 16          | L: 50          | R: 184.000     | FPS: 361.787   | T: 0:00:05    \n",
      "[08/29/2023 12:17:01 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 3072        | E: 24          | L: 62          | R: 140.000     | FPS: 467.296   | T: 0:00:06    \n",
      "[08/29/2023 12:17:02 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 4096        | E: 32          | L: 49          | R: 122.000     | FPS: 546.019   | T: 0:00:07    \n",
      "[08/29/2023 12:17:02 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Training Accomplished!\n",
      "[08/29/2023 12:17:02 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Model saved at: /export/yuanmingqi/code/rllte/examples/logs/ppo_atari/2023-08-29-12-16-54/model\n"
     ]
    }
   ],
   "source": [
    "from rllte.agent import PPO\n",
    "from rllte.env import make_atari_env\n",
    "from rllte.common.prototype import BaseEncoder\n",
    "from gymnasium.spaces import Space\n",
    "from torch import nn\n",
    "import torch as th\n",
    "\n",
    "class CustomEncoder(BaseEncoder):\n",
    "    \"\"\"Custom encoder.\n",
    "    \n",
    "    Args:\n",
    "        observation_space (Space): The observation space of environment.\n",
    "        feature_dim (int): Number of features extracted.\n",
    "\n",
    "    Returns:\n",
    "        The new encoder instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: Space, feature_dim: int = 0) -> None:\n",
    "        super().__init__(observation_space, feature_dim)\n",
    "\n",
    "        obs_shape = observation_space.shape\n",
    "        assert len(obs_shape) == 3\n",
    "\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[0], 32, 3, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        with th.no_grad():\n",
    "            sample = th.ones(size=tuple(obs_shape)).float()\n",
    "            n_flatten = self.trunk(sample.unsqueeze(0)).shape[1]\n",
    "\n",
    "        self.trunk.extend([nn.Linear(n_flatten, feature_dim), nn.ReLU()])\n",
    "\n",
    "    def forward(self, obs: th.Tensor) -> th.Tensor:\n",
    "        h = self.trunk(obs / 255.0)\n",
    "\n",
    "        return h.view(h.size()[0], -1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # env setup\n",
    "    device = \"cuda:0\"\n",
    "    env = make_atari_env(device=device)\n",
    "    eval_env = make_atari_env(device=device)\n",
    "    # create agent\n",
    "    feature_dim = 512\n",
    "    agent = PPO(env=env, \n",
    "                eval_env=eval_env, \n",
    "                device=device,\n",
    "                tag=\"ppo_atari\",\n",
    "                feature_dim=feature_dim)\n",
    "    # create a new encoder\n",
    "    encoder = CustomEncoder(observation_space=env.observation_space, \n",
    "                         feature_dim=feature_dim)\n",
    "    # set the new encoder\n",
    "    agent.set(encoder=encoder)\n",
    "    # start training\n",
    "    agent.train(num_train_steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15732c1-ab0c-4968-8e08-0a39061ae76b",
   "metadata": {},
   "source": [
    "We can find that the old `MnihCnnEncoder` has been replaced by `CustomEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100a194-97d3-4640-b291-d944ef4489b0",
   "metadata": {},
   "source": [
    "As for customizing modules like `Storage` and `Distribution`, etc., users should consider compatibility with specific algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# encoding: utf-8
# linktest/main.py
"""
linktest is a comprehensive automation test framework developed using Python3, Selenium, and Appium, designed to support API, UI, and Mobile automation testing.

What can linktest do?
- Provides support for UI, API, Mobile (iOS & Android) automation testing.
- Enables concurrent or sequential test case execution.
- Automatically re-runs failed test cases.
- Generates HTML reports automatically (stored under project/output).
- Allows setting individual timeouts for each test case, or utilizes settings.TESTCASE_TIMEOUT as the default.
- Offers multiple test case execution methods (see details below).
- Cross-platform compatibility (supports MacOS, Linux, Windows).
- Supports DataSet (CSV) integration.
- Support to specify the delimiter in CSV files.
- Facilitates setting Authentication TOKEN for UI test cases.
- Automatically installs the appropriate Chromedriver for UI test cases based on the Chrome version.
- Allows saving execution logs into testdb, which can be integrated with linktest-dashboard to display execution log summaries, details, and histories.
- Supports assigning multiple tags to a single test case for easy grouping.
- Allows setting priorities for test cases, offering another way to group test cases.
- Automatically store HTTP request and response data in the corresponding log file.
- Easily and directly save specified data to the GlobalDataList between different test cases (useful for aggregation and summary functions).
- integrate with test-engine-platform (easy to write the E2E testcases)

To utilize the linktest framework, your project must have two configuration files (these will be auto-generated if not present):

1. settings/__init__.py is used to configure the following items for linktest:
    - Environment (test environment: UAT, QA, PROD, etc.)
    - TESTCASE_TIMEOUT (if a test case's execution is not complete after Testcase_Timeout seconds, a TimeoutException will be thrown)
    - RERUN_FLAG (controls rerunning of failed test cases; default is False)
    - DEBUG_RUN (keeps the browser active after execution is done; default value is False)
    - Show_All_Chrome_Driver_Logs (determines whether to show all webdriver logs; default value is False)
    - QUEUE_SIZE (setting queue_size to 8, for example, means that a maximum of 8 test cases can be executed concurrently)
    - SAVE_LOG_TO_DB (saves execution logs into testdb; must be used in conjunction with linktest-dashboard, default value is False)
    - generate_xunit_result (saves xunit report; default value is False; used for TestLink integration)
    - AUTO_LOG_HTTP_REQUEST (default value is True; automatically logs request actions)
    - USE_JSON_INDENTATION (default value is True; uses JSON indentation in log)
    - AUTO_DOWNLOAD_CHROMEDRIVER (default value is False; automatically downloads the appropriate Chromedriver based on the Chrome version)
    - AUTO_SCREENSHOT_ON_ACTION (default value is False; automatically generates a screenshot for each WebDriver action)
    - DEFAULT_BROWSER_NAME (default browser name for UI test cases; default value is "chrome")
    - HEAD_LESS (default value is False; runs the browser in headless mode)
    - WEBDRIVER_IMPLICIT_WAIT (default value is 10 seconds; maximum time the WebDriver should wait for an element to be present before throwing an exception)
    - LOG_TO_CONSOLE (default value is True; always logs to the console)
    - ALWAYS_GENERATE_CURL (default value is False; always generates cURL commands)

    # ------ Configuration Related to the TestCase's Log ------
    - LOG_LEVEL = logging.DEBUG # default value is DEBUG
    - LOG_TO_CONSOLE = True # default value is True, Always log to file

2. settings/testcase_tags.py
    - The Tags class is used to define all test case tags.
    - Assign a tag to each test case (tags can be any non-blank string; you can create tags based on your specific needs, such as "smoke", "nightly", "regression", "p1", "ignore", etc.)
    - Note: If a test case's tag contains "ignore", it will only be added to the ignore_testcase_list, even if its tag attribute contains other tag names (e.g. tag = "nightly, smoke, ignore")

Usage of the linktest automation framework:

1. Install linktest by running the command: `pip install linktest`.

2. Copy the run.py file to your project:
    - run.py serves as the entry point for the linktest automation framework.
    - Your project structure should look like the following:
        ----project_name/     # project name
            ----common/       # common libraries
            ----settings/     # settings will be auto-generated by linktest if not present
            ----tests/        # all test cases must be under the "tests" package
            ----run.py        # the entry point of linktest

3. Write test cases (Frontend, Backend, or MobileTest) under your_project/tests/ - refer to the test case demo below.

4. The test case class must inherit from UITestCase, APITestCase, or a custom class that inherits from BaseTestCase.

5. All test cases must encapsulate their business logic within a specific method called run_test().

6. Assign a tag to each test case (tags can be any non-blank string; you can create tags based on your specific needs, such as "smoke", "nightly", "ignore", etc. For more details, please see setting/testcase_tags.py).

7. There are 7 ways to execute test cases:

   7.1 Run CMD: `python run.py tag_name` (provide the tag_name, and all test cases with the specified tag will be executed)
    or
   7.2 Run CMD: `python run.py TestCaseClassName` (provide the test case's class_name: TestCaseClassName (case-insensitive), and the specified test case will be executed)
    or
   7.3 Run CMD: `python run.py testcase_list_name` (provide the testcase_list_name defined in each package's __init__.py file)
    or
   7.4 Run CMD: `python run.py package_name` (provide the package_name, such as: tests.api or tests or tests.api.b1)
    or
   7.5 Run CMD: `python3 run.py priority=10` (run all test cases with priority set and value greater than or equal to 10)

   7.6 Run CMD: `python run.py tag1 tag2 testcase_list_1 testcase_list_2 TestcaseClassName1 package_name_1` (parameters for main.py can be any combination of the following four kinds, separated by a space:
                    1. tag_name  (e.g: "smoke" or "regression")
                    2. testcase's class_name (e.g: "TestBackendB1_1")
                    3. testcase_list_name  (e.g: "tests_backend_testcases" in __init__.py)
                    4. package_name (e.g: "tests" or "tests.backend" or "tests.ui")

    All test cases fetched by the given parameters will be added to testcase_list, and data deduplication will be performed.
    For each parameter, the search order is as follows: tag name, TestcaseClassName, TestcaseList, and package_name.

   +++ One More +++
   7.7 Run CMD: `python3 run.py env=DEV threads=10 tests.api` (start 10 threads to run test cases corresponding to tests.api in the DEV environment)

Tips:

1. Set testcase_id in run_test(), e.g:

        class Testcase1(APITestCase):
            tag = 'case1, smoke'
            def run_test(self):
                self.testcase_id = "case1"

  There are two methods to assign tags to test cases:
    1. tag = "smoke, nightly" - You can specify the tag as a string, with each tag name separated by a comma. It's crucial to verify the correct spelling of each tag name in all test cases to prevent typos (for instance, "nighty" instead of "nightly").
    2. tag = [Tags.Smoke, Tags.Nightly] - This is the recommended approach! Here, the tag is set as a list, with tag names defined in settings/testcase_tags.py.
    Note:
        1. If a tag includes "ignore", the corresponding test case will solely be appended to the ignore_testcase_list, irrespective of any other tags present.
        2. The interpretation of tag names is case-insensitive, meaning tags like "smoke" and "SMOKE" are considered identical.
        3. If a tag name incorporates a ".", it will be substituted with an "_". For instance, 'link.test' becomes 'link_test'.
2. Set a timeout (integer seconds) attribute for each test case to override settings.TESTCASE_TIMEOUT. If not set, the test case timeout will be the default value: settings.TESTCASE_TIMEOUT.
3. When settings.DEBUG_RUN is True, report.html will not be generated after each test case execution, improving performance since there is no "report_lock." For local test case execution or debugging, it is better to set settings.DEBUG_RUN = True.
4. The framework, using the auto_generate_testcase_list.py script, will automatically generate all __init__.py files under the tests package and create all test case lists grouped by tag name and package's path. As a result, you don't need to manually maintain the corresponding __init__.py files after adding, modifying, or deleting specific test case script files.
5. The framework, using the auto_generate_testcase_list_from_csv.py script, will automatically generate the corresponding test case script files that meet linktest requirements and their respective __init__.py files, based on the user-provided CSV file and its corresponding script files.
By following the above guidelines, you can effectively utilize the linktest automation framework to manage and execute test cases for UI, API, and mobile platforms. Be sure to understand the various ways of executing test cases and setting their attributes, such as tags and priority, to get the most out of the framework.

------ API Testcase demo ------:

from linktest.api_testcase import APITestCase

class Testcase1(APITestCase):
    tag = 'regression, smoke'
    priority = 10
    timeout = 20  # seconds

    def run_test(self):
        self.testcase_id = "case1"
        self.sleep(1)
        self.logger.info("Testcase1 executed...")
        self.GlobalData["username"] = "linktest"
        self.CaseData["username"] = "linktest"
        self.CaseData["phone"] = 110
        self.assert_equals(1, 1)
        self.requests.get("https://www.bing.com") # The framework will automatically save HTTP request and response data to the appropriate log file.



------ Frontend/UI Testcase demo ------:

from settings.testcase_tags import Tags
from linktest.frontend_testcase import UITestCase

class UITestcaseDemo(UITestCase):
    # set tag by way 1, this testcase will be auto added into smoke_testcase_list and nightly_testcase_list
    #tag = "smoke, nightly"

    # set tag by way 2, it's easy to know all the tag names in your project and will not type an error
    # tag name(eg. type "nighty" as "nightly"), recommend this way!
    # this testcase will be auto added into lin_testcase_list & smoke_testcase_list & nightly_testcase_lit
    tag = [Tags.Smoke, Tags.Nightly]

    # here set timeout for testcase, otherwise this testcase's timeout will be: settings.TESTCASE_TIMEOUT
    timeout = 10

    def run_test(self):
        self.testcase_id = "Testcase-1"
        self.logger.debug("load https://www.google.com")
        self.browser.get("https://www.google.com")

or:

class UITestcaseDemo(UITestCase):
    #tag = "nightly"  # this testcase will be auto added into nightly_testcase_list
    tag = [Tags.Nightly] # recommend this way

    # here set timeout for testcase, otherwise this testcase's timeout will be : settings.TESTCASE_TIMEOUT
    timeout = 10

    def run_test(self):
        self.logger.debug("load https://www.bing.com")
        self.browser.get("https://www.bing.com")

Note: if one testcase has multiple browsers, follow the below way, all the browsers will be closed after execution is done.

class FrontendTestcaseDemo(UITestCase):
    tag = [Tags.Ignore]  # this testcase will be auto added into ignore_testcase_list

    # here set timeout for testcase, otherwise this testcase's timeout will be: settings.TESTCASE_TIMEOUT
    timeout = 10

    def run_test(self):
        self.logger.debug("browser1 load https://www.bing.com")
        self.browser.get("https://www.bing.com")
        self.browser2 = SeleniumHelper("chrome")
        self.logger.debug("browser2 load https://www.github.com")
        self.browser2.get("https://www.github.com")

------ Mobile Testcase demo ------:

from settings.testcase_tags import Tags
from linktest.mobile_testcase import MobileTestCase

class MobileTestcaseDemo(MobileTestCase):
    # set tag by way 1, this testcase will be auto added into smoke_testcase_list and nightly_testcase_list
    #tag = "smoke, nightly"

    # set tag by way 2, it's easy to know all the tag names in your project and will not type an error
    # tag name(eg. type "nighty" as "nightly"), recommend this way!
    # this testcase will be auto added into lin_testcase_list & smoke_testcase_list & nightly_testcase_lit
    tag = [Tags.Smoke, Tags.Nightly]

    # here set timeout for testcase, otherwise this testcase's timeout will be: settings.TESTCASE_TIMEOUT
    timeout = 10

    def run_test(self):
        self.testcase_id = "Testcase-1"
        self.logger.debug("load mobile app")
        self.app_driver.launch_app()
or:

class MobileTestcaseDemo(MobileTestCase):
    #tag = "nightly"  # this testcase will be auto added into nightly_testcase_list
    tag = [Tags.Nightly] # recommend this way

    # here set timeout for testcase, otherwise this testcase's timeout will be : settings.TESTCASE_TIMEOUT
    timeout = 10

    def run_test(self):
        self.logger.debug("load mobile app")
        self.app_driver.launch_app()

------ Frontend/UI Testcase (with TOKEN) demo ------:

from linktest.ui_testcase import UITestCase

class OrderListWithToken(UITestCase):
    tag = "order, tc9527"

    token = "token-xxx"

    def run_test(self):
        self.browser.get("the-url-which-access-require-a-token")


-------------------------------- linktest DataSet/CSV Usage Start -----------------------------
If you need to use a dataSet (currently only CSV format is supported), please define it as follows:
    1. Define a CSV file
    2. In the same directory as the CSV file, define a .py file with the same name as the CSV file. This Python file must define a run_test(self, param...) method.
    Here's an example:

        1. def login(self, username, password):
               self.logger.info("login with username: %s, password: %s" % (username, password))

        2. Contents of the testLogin.csv (this CSV file uses a comma as the delimiter):
           username,password
           name1,password1
           name2,password2

        3. Contents of the `testLogin.py` file:
            # here should import login function
            def run_test(self, username, password):
                tag = "test_login"

                self.logger.info("start run_test()...")
                login(self, username, password)


    -----> If run correctly, the framework will automatically generate a standard file for framework execution, `xxx_auto_generated_by_csv.py` (in this case `testLogin_case_list_auto_generated_by_csv.py`), based on `testLogin.csv` and `testLogin.py`.
    Contents of `testLogin_case_list_auto_generated_by_csv.py`:

    from linktest.api_testcase import APITestCase
    from tests.api.csv_dataset import testLoginDataSet


    class testLoginDataSet_1(APITestCase):
        tag = "testLoginDataSet"

        def run_test(self):
            username = 'name1'
            password = 'password1'

            testLoginDataSet.run_test(self, username, password)


    class testLoginDataSet_2(APITestCase):
        tag = "testLoginDataSet"

        def run_test(self):
            username = 'name2'
            password = 'password2'

            testLoginDataSet.run_test(self, username, password)

-------------------------------- linktest DataSet/CSV Usage End --------------------------------


------ IOS Testcase Demo ------

import os
from settings.testcase_tags import Tags
from linktest.frontend_testcase import UITestCase
from linktest.ios_testcase import IOSTestCase

class IOSTestCaseDemo(IOSTestCase, UITestCase):
    tag = [Tags.Demo]

    # here set appium_server_ip for testcase, otherwise use default value: settings.appium_server_default_ip
    appium_server_ip = "127.0.0.1"

    # here set appium_server_port for testcase, otherwise use default value: settings.appium_server_default_port
    appium_server_port = "4729"

    ios_desired_capabilities = {
        'app': os.path.abspath('/path/test.app'),
        'platformName': 'iOS',
        'platformVersion': '9.3',
        'deviceName': 'iPhone 6'
    }

    def run_test(self):
        # self.logger is extends from BaseTestCase which was auto generated by framework
        self.logger.debug("start to run ios testcase")

        # self.ios_driver is extends from IOSTestCase which was auto generated and managed by framework
        self.ios_driver.switch_to_alert().accept()

        self.logger.debug("start to load https://www.google.com on PC")
        # self.browser is extends from UITestCase which was auto generated and managed by framework
        self.browser.get("https://www.google.com")


------ Android Testcase Demo ------

from linktest.android_testcase import AndroidTestCase

class AndroidTestCaseDemo(AndroidTestCase):
    tag = "android_testcase_demo"

    def run_test(self):
        user_name = self.android_driver.find_element_by_id("com.linktestapp:id/username")
        user_name.send_keys("user_name")
        password = self.android_driver.find_element_by_id("com.linktestapp:id/password")
        password.send_keys("test")

------ Define Multiple Testcase Classes In One Module ------
'''
LinkTest supports defining multiple Testcase Classes in one module.
Here, we define 3 test cases (Class Names) in the module: testcase_demo.py
'''
from settings.testcase_tags import Tags
from linktest.backend_testcase import APITestCase

class TestcaseDemo1(APITestCase):
    tag = [Tags.Smoke]

    def run_test(self):
        self.testcase_id = "1"
        self.logger.info("testcase TestcaseDemo1 start ...")

class TestcaseDemo2(APITestCase):
    tag = [Tags.Smoke]

    def run_test(self):
        self.testcase_id = "2"
        self.logger.info("testcase TestcaseDemo2 start ...")

class TestcaseDemo3(APITestCase):
    tag = [Tags.Smoke]

    def run_test(self):
        self.testcase_id = "3"
        self.logger.info("testcase TestcaseDemo3 start ...")


-------------

Tips:
1. Run `python3 run.py --help` to get detailed documentation for LinkTest.
2. In the case script's `run_test()` method, type "self." and take advantage of the IDE's auto-suggestion feature to explore more functions provided by the framework.


@author: Wang Lin, think_wl@163.com

"""

# from .framework_log import framework_logger

import importlib
import os
import time

try:
    import settings
except BaseException:
    raise ("No settings module found ....")

# set Default time_zone = 'Asia/Shanghai'
if getattr(settings, "TIME_ZONE", False):
    os.environ['TZ'] = '%s' % settings.TIME_ZONE
else:
    os.environ['TZ'] = 'Asia/Shanghai'

if hasattr(time, 'tzset'):
    # https://docs.python.org/3/library/time.html#time.tzset
    # Availability: Unix
    time.tzset()

import re
import sys
import json
import traceback
import time
import platform
import datetime
import logging
import threading
import collections
import subprocess
import functools
import linktest

linktestBanner = ''' _     _       _      _____         _   
| |   (_)_ __ | | __ |_   _|__  ___| |_ 
| |   | | '_ \| |/ /   | |/ _ \/ __| __|
| |___| | | | |   <    | |  __/\__ \ |_ 
|_____|_|_| |_|_|\_\   |_|\___||___/\__|
====================================================
:: linktest ::         (version %s)
''' % linktest.__version__

print(linktestBanner)

from .logged_requests import LoggedRequests


from . import get_project_info

project_info = get_project_info.get_project_info()

try:
    from . import doctor

    doctor.doctor_check()
except BaseException:
    traceback.print_exc()

from . import html_report
from . import generate_html_log
from . import run_testcase_thread
from . import timeout_thread
from . import testcase_timeout_exception
from . import auto_generate_testcase_list_from_csv
from . import auto_generate_testcase_list
from . import xml_report
from . import doctor
from . import base_testcase
from . import ui_testcase
from . import ios_testcase
from . import get_platform_info
from . import appium_utils
from . import android_testcase
from . import api_testcase
from .base_testcase import BaseTestCase
from .get_adb_devices import get_adb_devices
from .xml_report import convert_to_seconds

from urllib.error import URLError

PLATFORM_INFO = get_platform_info.get_platform_info()
DEFAULT_SLEEP_TIME_FOR_MOBILE_TESTCASE = 2
REAL_DEVICE = android_testcase.AndroidTestCase.REAL_DEVICE
VIRTUAL_DEVICE = android_testcase.AndroidTestCase.VIRTUAL_DEVICE

####### testcase log settings start #######
# Default values
log_format = '%(asctime)s %(levelname)s %(filename)s %(lineno)d: %(message)s'
# Read from settings
log_level = getattr(settings, 'LOG_LEVEL', logging.DEBUG)

####### testcase log settings end #######


class TestCaseExecutor(threading.Thread):
    failed_testcases = []
    passed_testcases = []
    error_testcases = []
    executing_ios_testcase_flag = False
    testcase_queue = collections.deque()
    report_lock = threading.Lock()
    appium_lock = threading.Lock()
    create_log_lock = threading.Lock()
    db_lock = threading.Lock()
    jenkins_job_name = None
    rerun_testcase_queue = None

    def __init__(self, testcase_queue, total_testcases_count, output_folder, begin_time):
        threading.Thread.__init__(self)
        self.output_folder = output_folder
        self.begin_time = begin_time
        self.queue = testcase_queue
        self.total_testcases_count = total_testcases_count
        TestCaseExecutor.testcase_queue = self.queue

        if getattr(settings, "RERUN_FLAG", False):
            TestCaseExecutor.rerun_testcase_queue = collections.deque()

    def create_logger(self, logger_ref, testcase_full_folder, logger_name):
        # Initialize handlers list
        handlers = []

        with TestCaseExecutor.create_log_lock:
            importlib.reload(logging)

            if not os.path.exists(testcase_full_folder):
                os.makedirs(testcase_full_folder)

            logfile_full_name = testcase_full_folder + os.sep + logger_name

            # always write log to file
            handlers.append(logging.FileHandler(logfile_full_name))

            # Add StreamHandler if LOG_TO_CONSOLE is set to True in settings
            if getattr(settings, 'LOG_TO_CONSOLE', True):
                handlers.append(logging.StreamHandler(sys.stdout))

            # Define basic configuration
            logging.basicConfig(
                # Define logging level
                level=log_level,
                # Declare the object we created to format the log messages
                format=log_format,
                # Declare handlers
                handlers=handlers
            )

            logger = logging.getLogger(logger_ref)

            return logger, logfile_full_name

    @staticmethod
    def get_device_name(testcase):
        device_name = testcase.__module__.split(".")[-1]
        if device_name.startswith(VIRTUAL_DEVICE):
            device_name = device_name.split(VIRTUAL_DEVICE)[1]
        elif device_name.startswith(REAL_DEVICE):
            device_name = device_name.split(REAL_DEVICE)[1]

        return device_name

    def run(self):
        while True:
            testcase = None
            if len(TestCaseExecutor.testcase_queue) == 0:
                # print("----------------- testcase queue is empty now! -----------------")
                if TestCaseExecutor.rerun_testcase_queue is not None:
                    if len(TestCaseExecutor.rerun_testcase_queue) == 0:
                        # print("----------------- rerun testcase queue is empty now! -----------------")
                        break
                    else:
                        testcase = TestCaseExecutor.rerun_testcase_queue.pop()
                        testcase.rerun_tag = 1
                else:
                    break
            else:
                testcase = TestCaseExecutor.testcase_queue.pop()

                # first run, set testcase.rerun_tag = 0
                testcase.rerun_tag = 0

            testcase.packages = ""
            tc_module_full_path = testcase.__module__

            if tc_module_full_path == "tests":
                testcase.packages = ",tests,"

            if "." in tc_module_full_path:
                package_list = tc_module_full_path.split(".")
                package_name_str_list = ["tests"]
                package_name_str_list_for_db = ""
                p_name_str = "tests"

                for package_name in package_list[1:]:
                    p_name_str += "." + package_name
                    package_name_str_list.append(p_name_str)

                for package_name in package_name_str_list:
                    package_name_str_list_for_db += package_name + ","

                testcase.packages = "," + package_name_str_list_for_db

            tags = getattr(testcase, "tag", "null")

            testcase.tag = ""

            if isinstance(tags, str):
                if tags == "null":
                    testcase.tag = tags
                else:
                    if "," in tags:
                        tags = tags.strip(",")
                        tags = tags.replace(" ", "")
                    testcase.tag = "," + tags.lower() + ","

            if isinstance(tags, list):
                if len(tags) == 0:
                    testcase.tag = "null"
                else:
                    for tag in tags:
                        testcase.tag += tag.lower().strip() + ","
                    testcase.tag = "," + testcase.tag

            if issubclass(testcase, ios_testcase.IOSTestCase):
                if TestCaseExecutor.executing_ios_testcase_flag is True:
                    TestCaseExecutor.testcase_queue.appendleft(testcase)
                    print(
                        "------ when TestcaseExecutor: %s running ----- already one ios testcase executing" % self)
                    time.sleep(DEFAULT_SLEEP_TIME_FOR_MOBILE_TESTCASE)
                    continue
                else:
                    TestCaseExecutor.executing_ios_testcase_flag = True

            if issubclass(testcase, android_testcase.AndroidTestCase):
                device_name = TestCaseExecutor.get_device_name(testcase)

                if getattr(TestCaseExecutor, device_name) is True:
                    TestCaseExecutor.testcase_queue.appendleft(testcase)
                    print(
                        "- already one android testcase executing on device: %s" % device_name)
                    time.sleep(DEFAULT_SLEEP_TIME_FOR_MOBILE_TESTCASE)
                    continue
                else:
                    setattr(TestCaseExecutor, device_name, True)

            try:
                executing_testcase = testcase()
                print("- Testcase: %s is running - " % executing_testcase)
                executing_testcase.timeout_flag = False
                executing_testcase.run_flag = False
                executing_testcase.done_flag = False
                executing_testcase.rerun_tag = testcase.rerun_tag

                if not hasattr(executing_testcase, "screenshot_list_for_db"):
                    executing_testcase.screenshot_path_list_for_db = []

                executing_testcase.ExecutionStatusSetByFramework = None

                if isinstance(executing_testcase, ui_testcase.UITestCase) or \
                        isinstance(executing_testcase, ios_testcase.IOSTestCase) or \
                        isinstance(executing_testcase, android_testcase.AndroidTestCase):
                    if executing_testcase.rerun_tag == 0:
                        executing_testcase.screenshot_name = "%s_screenshot.png" % testcase.__name__
                    else:
                        executing_testcase.screenshot_name = "%s_screenshot.png" % testcase.__name__
                        executing_testcase.rerun_screenshot_name = "%s_screenshot.rerun.png" % testcase.__name__

                executing_testcase.testcase_start_time = datetime.datetime.now()
                testcase_full_folder = self.output_folder + os.sep + testcase.__module__.replace(".", "_") + \
                                       os.sep + testcase.__name__

                if not os.path.exists(testcase_full_folder):
                    os.makedirs(testcase_full_folder)

                executing_testcase.full_tc_folder = testcase_full_folder

                if executing_testcase.rerun_tag == 0:
                    logger, logfile_full_name = self.create_logger(testcase.__name__,
                                                                   executing_testcase.full_tc_folder,
                                                                   "test.log")
                    executing_testcase.logger = logger
                    executing_testcase.requests = LoggedRequests(logger)

                    executing_testcase.logfile_full_name = logfile_full_name
                    executing_testcase.log_file_path = logfile_full_name.replace(project_info.output_folder, ".")

                else:
                    logger_rerun, logfile_full_name_rerun = self.create_logger(testcase.__name__,
                                                                               executing_testcase.full_tc_folder,
                                                                               "test.rerun.log")
                    executing_testcase.logger = logger_rerun
                    executing_testcase.requests = LoggedRequests(logger_rerun)


                    executing_testcase.logfile_full_name = logfile_full_name_rerun
                    executing_testcase.log_file_path = logfile_full_name_rerun.replace(project_info.output_folder, ".")

                executing_testcase.logger.info("- Test Host Operating System: %s" % PLATFORM_INFO + os.linesep)
                executing_testcase.logger.info(
                    "- Test Execution Environment: %s " % settings.ENVIRONMENT.upper() + os.linesep)

                # if IOS testcase run then should start appium first
                if issubclass(testcase, ios_testcase.IOSTestCase) or issubclass(testcase,
                                                                                android_testcase.AndroidTestCase):
                    from appium import webdriver as mobiledriver

                    xcode_version = get_platform_info.get_xcode_version()
                    print("xcode_version: %s" % xcode_version)

                    logfile_full_name_for_appium = executing_testcase.logfile_full_name.replace(".log", ".appium_log")

                    if hasattr(testcase, "appium_server_ip"):
                        appium_server_ip = testcase.appium_server_ip
                    else:
                        try:
                            appium_server_ip = settings.appium_server_default_ip
                        except AttributeError:
                            appium_server_ip = appium_utils.APPIUM_SERVER_DEFAULT_IP
                        testcase.appium_server_ip = appium_server_ip

                    if hasattr(testcase, "appium_server_port"):
                        appium_server_port = testcase.appium_server_port
                    else:
                        try:
                            appium_server_port = settings.appium_server_default_port
                        except AttributeError:
                            appium_server_port = appium_utils.APPIUM_SERVER_DEFAULT_PORT
                        testcase.appium_server_port = appium_server_port

                    testcase.appium_server_ip = appium_server_ip
                    testcase.appium_server_port = appium_server_port

                    # if testcase.rerun_tag == 1:
                    #     testcase.appium_server_port = str(int(testcase.appium_server_port) + 10000)

                    # because after start appium, need get the node process_id, so here add appium_lock
                    with TestCaseExecutor.appium_lock:
                        appium_utils.start_appium(testcase,
                                                  appium_log_path=logfile_full_name_for_appium,
                                                  appium_server_ip=testcase.appium_server_ip,
                                                  appium_server_port=testcase.appium_server_port)

                if issubclass(testcase, ios_testcase.IOSTestCase):
                    if hasattr(settings, "ios_desired_capabilities_in_settings_first"):
                        if settings.ios_desired_capabilities_in_settings_first is True:
                            if hasattr(settings, "ios_desired_capabilities"):
                                executing_testcase.logger.info(settings.ios_desired_capabilities)
                                testcase.ios_desired_capabilities = settings.ios_desired_capabilities
                            else:
                                if hasattr(testcase, "ios_desired_capabilities"):
                                    executing_testcase.logger.info(testcase.ios_desired_capabilities)
                                else:
                                    raise Exception(
                                        "There are no ios_desired_capabilities found in both testcase & settings, Please make sure provide ios_desired_capabilities in settings or testcase")
                        else:
                            if hasattr(testcase, "ios_desired_capabilities"):
                                executing_testcase.logger.info(testcase.ios_desired_capabilities)
                            else:
                                if hasattr(settings, "ios_desired_capabilities"):
                                    # executing_testcase.logger.info(settings.ios_desired_capabilities)
                                    testcase.ios_desired_capabilities = settings.ios_desired_capabilities
                                else:
                                    raise Exception(
                                        "There are no ios_desired_capabilities found in both testcase & settings, Please make sure provide ios_desired_capabilities in settings or testcase")
                    else:
                        if hasattr(testcase, "ios_desired_capabilities"):
                            executing_testcase.logger.info(testcase.ios_desired_capabilities)
                        else:
                            if hasattr(settings, "ios_desired_capabilities"):
                                executing_testcase.logger.info(settings.ios_desired_capabilities)
                                testcase.ios_desired_capabilities = settings.ios_desired_capabilities
                            else:
                                raise Exception(
                                    "There are no ios_desired_capabilities found in both testcase & settings, Please make sure provide ios_desired_capabilities in settings or testcase")

                    if int(xcode_version.split(".")[0]) >= 8:
                        if "automationName" not in testcase.ios_desired_capabilities.keys():
                            testcase.ios_desired_capabilities["automationName"] = "XCUITest"

                    if "platformName" not in testcase.ios_desired_capabilities.keys():
                        testcase.ios_desired_capabilities["platformName"] = "ios"

                    if not "platformVersion" in testcase.ios_desired_capabilities.keys():
                        testcase.ios_desired_capabilities[
                            "platformVersion"] = get_platform_info.get_ios_platform_version()

                    if "app" in testcase.ios_desired_capabilities.keys():
                        app_path = testcase.ios_desired_capabilities["app"]
                        if app_path.startswith("/"):
                            # if the app_path starts_with "/", then it means user set the absolute path, just pass it.
                            pass
                        else:
                            if app_path.startswith("app/"):
                                testcase.ios_desired_capabilities["app"] = project_info.project_path + "/" + app_path

                    executing_testcase.ios_driver = mobiledriver.Remote(
                        'http://%s:%s/wd/hub' % (testcase.appium_server_ip, testcase.appium_server_port),
                        desired_capabilities=testcase.ios_desired_capabilities)
                    executing_testcase.logger.info(
                        '- IOS Driver Info: %s' % executing_testcase.ios_driver.__dict__ + os.linesep)

                    if hasattr(settings, "ios_implicitly_wait"):
                        if type(settings.ios_implicitly_wait) == int or type(settings.ios_implicitly_wait) == float:
                            executing_testcase.ios_driver.implicitly_wait(getattr(settings, "ios_implicitly_wait"))
                            executing_testcase.logger.info("set ios_implicitly_wait:%s" % settings.ios_implicitly_wait)
                        else:
                            # if the type of settings.ios_implicitly_wait is not correct, here set a default value: 60
                            executing_testcase.ios_driver.implicitly_wait(60)
                            print(
                                "the type of settings.ios_implicitly_wait:%s is not 'int' or 'float', here set a default value: 60" % settings.ios_implicitly_wait)
                            executing_testcase.logger.warning(
                                "the type of settings.ios_implicitly_wait:%s is not 'int' or 'float', here set a default value: 60" % settings.ios_implicitly_wait)
                    else:
                        # if there are no ios_implicitly_wait found in settings, then set a default value: 60
                        executing_testcase.ios_driver.implicitly_wait(60)
                        print(
                            "there are no ios_implicitly_wait found in settings, then set a default value: 60")
                        executing_testcase.logger.warning(
                            "there are no ios_implicitly_wait found in settings, then set a default value: 60")

                if issubclass(testcase, android_testcase.AndroidTestCase):
                    if hasattr(testcase, "logger"):
                        if hasattr(testcase, "android_desired_capabilities"):
                            executing_testcase.logger.info(testcase.android_desired_capabilities)
                        else:
                            executing_testcase.logger.info(settings.android_desired_capabilities)
                            testcase.android_desired_capabilities = settings.android_desired_capabilities

                    executing_testcase.android_driver = mobiledriver.Remote(
                        'http://%s:%s/wd/hub' % (testcase.appium_server_ip, testcase.appium_server_port),
                        desired_capabilities=testcase.android_desired_capabilities)

                    if hasattr(settings, "android_implicitly_wait"):
                        if type(settings.android_implicitly_wait) == int or type(
                                settings.android_implicitly_wait) == float:
                            executing_testcase.android_driver.implicitly_wait(
                                getattr(settings, "android_implicitly_wait"))
                            executing_testcase.logger.info(
                                "set android_implicitly_wait:%s" % settings.android_implicitly_wait)
                        else:
                            # if the type of settings.android_implicitly_wait is not correct, here set a default value: 60
                            print(
                                "the type of settings.android_implicitly_wait:%s is not 'int' or 'float', here set a default value: 60" % settings.android_implicitly_wait)
                            executing_testcase.logger.warning(
                                "the type of settings.android_implicitly_wait:%s is not 'int' or 'float', here set a default value: 60" % settings.android_implicitly_wait)
                    else:
                        # if there are no android_implicitly_wait found in settings, then set a default value: 60
                        executing_testcase.android_driver.implicitly_wait(60)
                        print(
                            "there are no android_implicitly_wait found in settings, then set a default value: 60")
                        executing_testcase.logger.warning(
                            "there are no android_implicitly_wait found in settings, then set a default value: 60")

                    executing_testcase.logger.info(
                        '- Android Driver Info: %s' % executing_testcase.android_driver.__dict__ + os.linesep)

                # Backend testcases will not need to create browser & close browser
                if issubclass(testcase, ui_testcase.UITestCase):
                    try:
                        executing_testcase.create_browser_driver()
                        testcase.browser = executing_testcase.browser
                        testcase.browserName = executing_testcase.browser.name
                        executing_testcase.logger.info(
                            "- Browser: %s - Version:%s " % (
                                testcase.browserName.upper(),
                                testcase.browser.capabilities['browserVersion']) + os.linesep)
                    except URLError:
                        traceback.print_exc()
                        raise BaseException("open browser error")

                # todo: DEBUG run 模式 真需要吗？！
                if not settings.DEBUG_RUN:
                    # if settings.DEBUG_RUN is False, sometimes maybe the
                    # browser will in "No response" Status, so need to set timeout for each testcase.
                    # here start 2 daemon thread to implement the testcase's timeout function

                    testcase.timeout_flag = False
                    testcase.run_flag = False
                    testcase.done_flag = False

                    testcase_timeout_thread = timeout_thread.TimeoutThread(executing_testcase)
                    testcase_timeout_thread.setDaemon(True)
                    testcase_timeout_thread.start()

                    if hasattr(executing_testcase, "exception_info"):
                        # before testcase start to run, set its exception_info as None
                        executing_testcase.exception_info = None

                    while executing_testcase.timeout_flag is False and executing_testcase.done_flag is False:
                        if executing_testcase.run_flag is False:
                            executing_testcase.run_flag = True

                            testcase_run_testcase_thread = run_testcase_thread.StartTestcaseThread(executing_testcase)
                            testcase_run_testcase_thread.setDaemon(True)
                            testcase_run_testcase_thread.start()

                            if hasattr(executing_testcase, "exception_info"):
                                if executing_testcase.exception_info is not None:
                                    raise Exception(executing_testcase.exception_info)

                        if hasattr(executing_testcase, "exception_info"):
                            if executing_testcase.exception_info is not None:
                                raise Exception(executing_testcase.exception_info)

                    if executing_testcase.timeout_flag is True:
                        raise testcase_timeout_exception.TestCaseTimeoutException("TimeoutException")
                else:
                    executing_testcase.testcase_start_time = datetime.datetime.now()

                    # 执行 setup() 之前 先 log TestEngineCaseInput(如果是 test-engine 发起的)
                    if BaseTestCase.TestEngineCaseInput:
                        executing_testcase.logger.info("- TestEngineCaseInput:")
                        executing_testcase.logger.info(executing_testcase.TestEngineCaseInput)

                    # 先执行 setup()， 再执行 run_test()
                    if hasattr(executing_testcase, "setup"):
                        executing_testcase.logger.info(" - setup() Start execution")
                        executing_testcase.setup()
                        executing_testcase.logger.info(" - setup() End execution")

                    executing_testcase.logger.info(" - run_test() Start execution")

                    # For DEBUG_RUN mode,will not set timeout for each testcase
                    executing_testcase.run_test()

            except BaseException:
                executing_testcase.ExecutionStatusSetByFramework = "failed"
                traceback.print_exc()
                traceback_info = traceback.format_exc()

                # if issubclass(testcase, ui_testcase.UITestCase):
                #
                #     try:
                #         # save the current URL and all the browser's related information when got error.
                #         executing_testcase.logger.error(
                #             "---------------- Error URL & Browser Info Start --------------" +
                #             executing_testcase.browser.current_url() + os.linesep)
                #         executing_testcase.logger.error(testcase.browser.__dict__)
                #         executing_testcase.logger.error(
                #             "---------------- Error URL & Browser Info End --------------" + os.linesep)
                #     except BaseException:
                #         # todo: MacOS executing_testcase.browser.current_url() windows: ?
                #         pass

                executing_testcase.logger.error(traceback_info)

                if executing_testcase.timeout_flag is True:
                    exception_info = "TestCaseTimeoutException"
                    traceback_info_str = "TestcaseTimeout"
                else:
                    traceback_info_reversed = []

                    for line in traceback_info.strip().splitlines():
                        traceback_info_reversed.insert(0, line)

                    for traceback_info_str in traceback_info_reversed:
                        exception_info = traceback_info_str.split(":")[0]

                        if exception_info.__contains__("Exception") or exception_info.__contains__("Error"):
                            # remove the html tag in execption_info
                            exception_info = re.sub(r'</?\w+[^>]*>', '', exception_info)
                            break

                executing_testcase.exception_info = exception_info
                executing_testcase.exception_info_for_xml_report = re.sub(r'</?\w+[^>]*>', '', traceback_info_str)
                executing_testcase.error_message_for_db = executing_testcase.exception_info_for_xml_report
                executing_testcase.traceback_info = traceback_info

                TestCaseExecutor.failed_testcases.append(executing_testcase)

                if TestCaseExecutor.rerun_testcase_queue is not None:
                    if executing_testcase.rerun_tag == 0:
                        TestCaseExecutor.rerun_testcase_queue.append(testcase)
                        print("TestCase: %s run got failed, Re-run it later..." % testcase.__name__)

                with open(executing_testcase.logfile_full_name, "r") as case_log_file:
                    log_file_content = case_log_file.read()
                    executing_testcase.log_file_content = log_file_content

                try:
                    # 只有通过test-engine 触发的end2end testcase 才需要由框架统一调用 test-engine callback-api
                    # if hasattr(TestCaseExecutor.TestEngineCaseInput, "execution_id"):
                    if "execution_id" in executing_testcase.TestEngineCaseInput.keys() and \
                            executing_testcase.TestEngineCaseInput["execution_id"]:
                        # print(executing_testcase.TestEngineCaseInput["execution_id"])

                        headers = {
                            "Content-Type": "application/json",
                            "Host": "%s" % settings.TestEngineCallBackHost
                        }

                        for key in executing_testcase.TestEngineCaseOutput:
                            if type(executing_testcase.TestEngineCaseOutput[key]) is str or\
                                    type(executing_testcase.TestEngineCaseOutput[key]) is int:
                                pass
                            else:
                                raise Exception("The type of %s can only be str or int " % executing_testcase.TestEngineCaseOutput[key])

                        log_path = project_info.project_name + executing_testcase.logfile_full_name.split(
                            project_info.project_name + os.sep + "output")[-1]

                        callback_post_data = {
                            "chainDataId": executing_testcase.TestEngineCaseInput["execution_id"],
                            "currentCaseIndex": executing_testcase.TestEngineCaseInput["case_index"],
                            "respBody": executing_testcase.TestEngineCaseOutput,
                            "reqBody": executing_testcase.TestEngineCaseInput,
                            # "caseExecutionLog": log_path,
                            "caseExecutionLog": executing_testcase.log_file_content,
                            "testExecutionStatus": "FAILED"
                        }


                        res = executing_testcase.requests.post(
                            "%s/case/exec/complete" % settings.TestEngineCallBackAPI,
                            data=json.dumps(callback_post_data),
                            headers=headers)
                except BaseException:
                    traceback.print_exc()
                    executing_testcase.logger.error(traceback.format_exc())
                    pass
            else:
                # TestCaseExecutor.passed_testcases.append(testcase)
                TestCaseExecutor.passed_testcases.append(executing_testcase)

                for index, failed_tc in enumerate(TestCaseExecutor.failed_testcases):
                    if type(executing_testcase) == type(failed_tc):
                        TestCaseExecutor.failed_testcases.pop(index)
                        break

                try:
                    # 只有通过test-engine 触发的end2end testcase 才需要由框架统一调用 test-engine callback-api
                    # if hasattr(executing_testcase.TestEngineCaseInput, "execution_id"):
                    if "execution_id" in executing_testcase.TestEngineCaseInput.keys() and \
                            executing_testcase.TestEngineCaseInput["execution_id"]:

                        headers = {
                            "Content-Type": "application/json",
                            "Host": "%s" % settings.TestEngineCallBackHost
                        }

                        with open(executing_testcase.logfile_full_name, "r") as case_log_file:
                            log_file_content = case_log_file.read()
                            executing_testcase.log_file_content = log_file_content

                        for key in executing_testcase.TestEngineCaseOutput:
                            if type(executing_testcase.TestEngineCaseOutput[key]) is str or\
                                    type(executing_testcase.TestEngineCaseOutput[key]) is int:
                                pass
                            else:
                                raise Exception("The type of %s can only be str or int " % executing_testcase.TestEngineCaseOutput[key])

                        log_path = project_info.project_name + executing_testcase.logfile_full_name.split(
                            project_info.project_name + os.sep + "output")[-1]
                        print("- will callback log_path")
                        print(log_path)

                        callback_post_data = {
                            "chainDataId": executing_testcase.TestEngineCaseInput["execution_id"],
                            "currentCaseIndex": executing_testcase.TestEngineCaseInput["case_index"],
                            "respBody": executing_testcase.TestEngineCaseOutput,
                            "reqBody": executing_testcase.TestEngineCaseInput,
                            # "caseExecutionLog": log_path,
                            "caseExecutionLog": executing_testcase.log_file_content,
                            "testExecutionStatus": "PASSED"
                        }

                        res = executing_testcase.requests.post(
                            "%s/case/exec/complete" % settings.TestEngineCallBackAPI,
                            data=json.dumps(callback_post_data),
                            headers=headers)
                except BaseException:
                    traceback.print_exc()
                    executing_testcase.logger.error("\n\n")
                    executing_testcase.logger.error("******************************** Invoke TestEngine Callback ******")
                    executing_testcase.logger.error("********************************  Error  *************************")
                    executing_testcase.logger.error(traceback.format_exc())
                    pass
            finally:
                # 如果case中 有调用： self.GlobalDataList.append("XXX") 则推荐 settings中设置 ReRun_flag = False
                executing_testcase.GlobalExecutedCaseList.append(executing_testcase)

                try:
                    if hasattr(executing_testcase, "teardown"):
                        executing_testcase.logger.info(" - teardown() Start execution")
                        executing_testcase.teardown()
                        executing_testcase.logger.info(" - teardown() End execution")
                except BaseException as e:
                    executing_testcase.logger.error(e)

                try:
                    if hasattr(executing_testcase, "testcase_id"):
                        testcase.testcase_id = executing_testcase.testcase_id
                    elif hasattr(executing_testcase, "test_case_id"):
                        executing_testcase.logger.warning(
                            "******** Warning: 'test_case_id' has been deprecated, please change to 'testcase_id' ********")
                        testcase.testcase_id = executing_testcase.test_case_id
                    else:
                        testcase.testcase_id = "None"

                    executing_testcase.logger.handlers = []
                    testcase_end_time = datetime.datetime.now()
                    testcase_execution_time = testcase_end_time - executing_testcase.testcase_start_time

                    executing_testcase.execution_time = convert_to_seconds(testcase_execution_time)

                    # backend testcase will not have to create browser & close browser
                    if issubclass(testcase, ui_testcase.UITestCase) or \
                            issubclass(testcase, ios_testcase.IOSTestCase) or \
                            issubclass(testcase, android_testcase.AndroidTestCase):
                        from selenium.webdriver.remote.webdriver import WebDriver

                        webdriver_list = []

                        # sometimes one testcase will open multiple browsers(eg. teacher & students), here will fetch all the
                        # testcase's browsers and save screenshot for all browsers then close them.
                        for key, val in executing_testcase.__dict__.items():
                            if isinstance(val, WebDriver):
                                webdriver_list.append(key)

                        close_browser_after_done_flag = True;
                        if hasattr(settings, "CLOSE_BROWSER"):
                            close_browser_after_done_flag = settings.CLOSE_BROWSER

                        for browser in webdriver_list:
                            webdriver = getattr(executing_testcase, browser)

                            try:
                                if executing_testcase.ExecutionStatusSetByFramework == "failed":
                                    if executing_testcase.rerun_tag == 0:
                                        webdriver.save_screenshot(
                                            testcase_full_folder + os.sep + browser + "_" +
                                            executing_testcase.screenshot_name)
                                        executing_testcase.screenshot_path_list_for_db.append(
                                            testcase_full_folder + os.sep + browser + "_" +
                                            executing_testcase.screenshot_name)
                                    elif executing_testcase.rerun_tag == 1:
                                        webdriver.save_screenshot(
                                            executing_testcase.full_tc_folder + os.sep + browser + "_" +
                                            executing_testcase.rerun_screenshot_name)
                                elif hasattr(settings, "SAVE_SCREENSHOT_FOR_PASSED_TESTS"):
                                    if settings.SAVE_SCREENSHOT_FOR_PASSED_TESTS:
                                        if executing_testcase.rerun_tag == 0:
                                            webdriver.save_screenshot(
                                                testcase_full_folder + os.sep + browser + "_" +
                                                executing_testcase.screenshot_name)
                                            executing_testcase.screenshot_path_list_for_db.append(
                                                testcase_full_folder + os.sep + browser + "_" +
                                                executing_testcase.screenshot_name)

                                            executing_testcase.screenshot = testcase_full_folder.replace(
                                                project_info.output_folder, ".") \
                                                                            + os.sep + browser + "_" + \
                                                                            executing_testcase.screenshot_name
                                        elif executing_testcase.rerun_tag == 1:
                                            webdriver.save_screenshot(
                                                executing_testcase.full_tc_folder + os.sep + browser + "_"
                                                + executing_testcase.rerun_screenshot_name)

                                            executing_testcase.rerun_screenshot = testcase_full_folder.replace(
                                                project_info.output_folder,
                                                ".") + os.sep + browser + "_" + executing_testcase.rerun_screenshot_name

                            except BaseException:
                                traceback.print_exc()
                            finally:
                                try:
                                    is_appium_installed = True
                                    try:
                                        import appium
                                    except ImportError:
                                        is_appium_installed = False

                                    if is_appium_installed is True:
                                        # if IOS testcase or Andriod Testcase
                                        if (issubclass(executing_testcase.__class__, ios_testcase.IOSTestCase) or
                                            issubclass(executing_testcase.__class__, android_testcase.AndroidTestCase)) \
                                                and isinstance(webdriver, appium.webdriver.webdriver.WebDriver):
                                            try:
                                                webdriver.close_app()
                                            except BaseException:
                                                traceback.print_exc()
                                            try:
                                                if hasattr(testcase, "node_process_pid"):
                                                    if platform.system() == "Windows":
                                                        os.system("taskkill /f /im %s" % testcase.node_process_pid)
                                                    else:
                                                        os.system("kill -9 %s" % testcase.node_process_pid)
                                                        # todo: delattr(testcase, "node_process_pid") ? then no need blew one line code?
                                                    testcase.process_id_testcase_dict.pop(testcase.node_process_pid)
                                            except BaseException:
                                                traceback.print_exc()
                                        elif isinstance(webdriver, WebDriver):
                                            if close_browser_after_done_flag:
                                                webdriver.delete_all_cookies()
                                                webdriver.quit()
                                    elif isinstance(webdriver, WebDriver):
                                        if close_browser_after_done_flag:
                                            webdriver.delete_all_cookies()
                                            webdriver.quit()
                                except BaseException:
                                    traceback.print_exc()
                                

                        if issubclass(testcase, ios_testcase.IOSTestCase):
                            # set executing_ios_testcase_flag as False after this ios testcase execution done
                            TestCaseExecutor.executing_ios_testcase_flag = False  # todo
                            kill_process_by_name("Simulator")

                        if issubclass(testcase, android_testcase.AndroidTestCase):
                            setattr(TestCaseExecutor, device_name, False)
                except BaseException:
                    traceback.print_exc()
                finally:
                    try:
                        if issubclass(testcase, ios_testcase.IOSTestCase):
                            # set executing_ios_testcase_flag as False after this ios testcase execution done
                            TestCaseExecutor.executing_ios_testcase_flag = False

                        if issubclass(testcase, android_testcase.AndroidTestCase):
                            setattr(TestCaseExecutor, device_name, False)

                        with open(executing_testcase.logfile_full_name, "r", encoding="utf-8",
                                  errors="ignore") as logfile_full_name:
                            execution_log = logfile_full_name.read()
                            executing_testcase.execution_log = execution_log

                    finally:
                        try:
                            generate_html_log.generate_html_log(executing_testcase)
                        except BaseException:
                            traceback.print_exc()

                        if os.sep + "jenkins" + os.sep + "workspace" + os.sep in project_info.project_path:
                            TestCaseExecutor.jenkins_job_name = \
                                project_info.project_path.split(os.sep + "jenkins" + os.sep + "workspace" + os.sep)[
                                    1].split(os.sep)[0]

                try:
                    print(" - (Passed:%s | Failed:%s | Remaining:%s | Total:%s) - Testcase execution for '%s' has completed." % (
                            len(TestCaseExecutor.passed_testcases),
                            int(len(TestCaseExecutor.failed_testcases)/2) if getattr(settings, "RERUN_FLAG", False) else len(TestCaseExecutor.failed_testcases),
                            len(TestCaseExecutor.testcase_queue),
                            self.total_testcases_count,
                            testcase.__name__))

                except BaseException:
                    pass


def kill_process_by_name(process_name):
    for i in range(5):
        try:
            if platform.system() == "Windows":
                subprocess.check_output("taskkill /f /im %s" % process_name, shell=True, stderr=subprocess.STDOUT)
            else:
                subprocess.check_output("killall '%s'" % process_name, shell=True, stderr=subprocess.STDOUT)

            print("kill process: %s" % process_name)

        except BaseException as e:
            pass


def clean_appium_env():
    if platform.system() == "Windows":
        kill_process_by_name("node.exe")
    else:
        kill_process_by_name("node")
        kill_process_by_name("instruments")
        kill_process_by_name("Simulator")


def close_browsers_and_webdrivers():
    if platform.system() == "Windows":
        kill_process_by_name("chrome.exe")
        kill_process_by_name("chromedriver.exe")
        kill_process_by_name("firefox.exe")
        kill_process_by_name("iexplore.exe")
        kill_process_by_name("IEDriverServer.exe")
        kill_process_by_name("node.exe")
    else:
        # kill_process_by_name("Google Chrome")
        kill_process_by_name("chromedriver")
        kill_process_by_name("node")
        kill_process_by_name("instruments")
        kill_process_by_name("Simulator")


def usage():
    print(os.linesep + "-------------------------------- linktest Automation Framework Usage Start -----------------------------")
    print(__doc__)
    print("-------------------------------- linktest Automation Framework Usage End --------------------------------" + os.linesep)

def csv_usage():
    print(os.linesep + "-------------------------------- linktest DataSet/CSV Usage Start -----------------------------")
    message = (
        f" LinkTest 的 DataSet 功能的标准要求: \n"
        f"  1. CSV 文件及其对应的 Python 文件的文件名必须保持一致\n"
        f"  2. Python 文件中必须定义 `run_test(self, ...)` 方法, 并且参数列表中除了 `self` 之外, 必须至少还有一个其他参数\n"
        f"  3. Python 文件中定义的 `run_test()` 方法的所有参数名（除 `self` 参数外), 都必须存在于对应的 CSV 文件的表头（header）中\n"
    )
    print(message)

    print("""如果需要使用dataSet(目前只支持CSV格式), 请按照如下方式定义：
    1. 定义一个 csv文件
    2. 在csv文件的同级目录下定义一个和csv文件同名的.py文件, py文件中必须定义一个 run_test(self, param...)方法
    参考如下：
    
        1. def login(self, username, password):
               self.logger.info("login with username: %s, password: %s" % (username, password))
    
        2. testLogin.csv 文件如下(该csv文件以英文逗号作为分隔符)：
           username,password
           name1,password1
           name2,password2
        
        3. testLogin.py文件内容如下：
            # here should import login function 
            def run_test(self, username, password):
                tag = "test_login"
        
                self.logger.info("start run_test()...")
                login(self, username, password)

    
    -----> 如果正确运行后，框架会根据 testLogin.csv & testLogin.py 自动生成符合框架运行标准的 xxx_auto_generated_by_csv.py文件（testLogin_case_list_auto_generated_by_csv.py）
    testLogin_case_list_auto_generated_by_csv.py 内容如下：
    
    from linktest.api_testcase import APITestCase
    from tests.api.csv_dataset import testLoginDataSet
    
    
    class testLoginDataSet_1(APITestCase):
        tag = "testLoginDataSet"
    
        def run_test(self):
            username = 'name1'
            password = 'password1'
    
            testLoginDataSet.run_test(self, username, password)
    
    
    class testLoginDataSet_2(APITestCase):
        tag = "testLoginDataSet"
    
        def run_test(self):
            username = 'name2'
            password = 'password2'
    
            testLoginDataSet.run_test(self, username, password)
    """)
    print("-------------------------------- linktest DataSet/CSV Usage End --------------------------------" + os.linesep)

def update_config(args):
    """
    update_config.py is used to update the settings/__init__.py

    Note:
        the argv must be like "key=val key2=val2", each argv split by white-space and the key is case-insensitive

    @author: Wang Lin
    """

    import os

    from .get_project_info import get_project_info

    project_info = get_project_info()
    settings_file_path = project_info.project_path + os.sep + "settings" + os.sep + "__init__.py"

    argv_list = []

    for item in args.split("&"):
        item = item.lstrip().rstrip()
        if item.find("=") != -1:
            argv_list.append(item)

    if len(argv_list) > 0:
        for argv in argv_list:
            if "=" not in argv:
                print("the argv must like: key=val")

        argv_dict = {}
        updated_argv = []

        for argv in argv_list:
            if (argv.find("=") != -1):
                argv_dict[argv.split("=")[0].lstrip().rstrip()] = argv.split("=")[1].lstrip().rstrip()

        with open(settings_file_path, "r") as settings_file:
            contents = settings_file.readlines()
            i = 0

            while i < len(contents):
                line = contents[i]
                line_remove_whitespace = line.replace(" ", "")

                for argv in argv_dict.keys():
                    if line_remove_whitespace.split("=")[0].lower().lstrip().rstrip() == argv.lower().lstrip().rstrip():
                        contents[i] = "%s = %s \n" % (
                            line_remove_whitespace.split("=")[0].lstrip().rstrip(), argv_dict[argv].lstrip().rstrip())
                        print("UPDATE in settings" + os.sep + "__init__.py: %s " % contents[i])
                        updated_argv.append(argv)
                        break
                i += 1

            with open(settings_file_path, "w") as settings_new_file:
                settings_new_file.writelines(contents)

        if len(updated_argv) < len(argv_list):
            argv_list_not_found = []

            for argv in argv_dict.keys():
                if argv not in updated_argv:
                    argv_list_not_found.append(argv)

            print("***************** WARNING: below argv not found in settings/__init__.py *****************")
            print(argv_list_not_found)

            for nof_found_arv in argv_list_not_found:
                print("***************** WARNING: %s *****************" % nof_found_arv)
                with open(settings_file_path, "a") as settings_new_file:
                    print("%s = %s \n" % (nof_found_arv, argv_dict[nof_found_arv]))

                    settings_new_file.writelines("\n%s = %s \n" % (nof_found_arv, argv_dict[nof_found_arv]))

        try:
            importlib.reload(settings)
        except BaseException:
            traceback.print_exc()


def run_with(args=None):
    """
    the args for func run_with(args=None) support below 3 types:
        1. None
            e.g: main.run_with()
        2. string which support below 4 types:
            1. tag_name
                e.g: main.run_with("smoke")
            2. testcase's class_name
                e.g: main.run_with("TestBackend_1")
            3. testcase_list_name
                e.g: main.run_with("tests_backend_testcases")
            4. package_name
                e.g: main.run_with("tests.backend")
            5. the string can be any combination of the above four kinds which separated by a whitespace
                e.g: main.run_with("smoke TestBackend_1 testcase_list_1 tests.backend")
        3. a list which element's type is String
            e.g:
                main.run_with(["TestCase_1", "tag_name", "tests.backend"])
              or
                main.run_with("TestCase_1 tag_name tests.backend".split())
              or
                main.run_with("TestAPITestcase".split())

    return: output_folder, last_executed_case_log_full_path
    """
    print("====== init args:")
    print(args)

    if args is None:
        args = []
    elif isinstance(args, str):
        args = args.replace("{ ", "{").replace(" }", "}").replace(": ", ":").replace(" :", ":").replace(", ", ",").replace(" ,", ",").split()
    elif isinstance(args, list):
        for argv in args:
            if not isinstance(argv, str):
                raise TypeError(run_with.func_doc)
    else:
        raise TypeError(run_with.func_doc)

    # here remove the file name when execute run_test() from CMD
    if len(args) > 0:
        if args[0].__contains__(".py"):
            args = args[1:]

    print("init args after format:")
    print(args)

    # python3 run.py testcase_id=tc001 env=DEV thread=1 params="{'username':'lin','caseno': 50}" execution_id=10001 TESTCASE_TIMEOUT=10 DEBUG_RUN=False RERUN_FLAG=True
    update_config_str = ""
    params_str = None
    execution_id = None
    project_id = None
    case_id = None
    case_index = None
    callback_api = None

    priority = None

    for item in args:
        if item.find("=") != -1:
            item, value = item.split('=')[0].upper().lstrip().rstrip(), item.split('=')[1]
            print(item, "=", value)
            if item == '' or value == '':
                continue

            if item == 'ENV' or item == 'ENVIRONMENT':
                if value.lower() == 'dev':
                    update_config_str += 'ENVIRONMENT = Environment.DEV &'
                if value.lower() == 'qa':
                    update_config_str += 'ENVIRONMENT = Environment.QA & '
                if value.lower() == 'uat':
                    update_config_str += 'ENVIRONMENT = Environment.UAT & '
                if value.lower() == 'sit':
                    update_config_str += 'ENVIRONMENT = Environment.SIT &'
                if value.lower() == 'prod':
                    update_config_str += 'ENVIRONMENT = Environment.PROD &'

            elif item == 'THREADS' or item == 'THREAD' or item == 'QUEUE_SIZE':
                update_config_str += 'QUEUE_SIZE = %s &' % value
            elif item == 'PARAMS':
                params_str = value
            elif item == 'EXECUTIONID' or item == 'EXECUTION_ID':
                execution_id = value.lstrip().rstrip()
            elif item == 'PROJECTID' or item == 'PROJECT_ID':
                project_id = value.lstrip().rstrip()
            elif item == 'CALLBACK_API' or item == 'CALLBACKAPI':
                callback_api = value.lstrip().rstrip()
            elif item == 'CASEID' or item == "CASE_ID" or item == "TESTCASE_ID" or item == "TESTCASEID":
                case_id = value.lstrip().rstrip()
            elif item == 'CASEINDEX' or item == "CASE_INDEX":
                case_index = value.lstrip().rstrip()
            elif item == 'PRIORITY':
                priority = value.lstrip().rstrip()
                priority = int(priority)
            else:
                update_config_str += "%s=%s &" % (item.lstrip().rstrip(), value)

    print("type(execution_id):", type(execution_id))
    print("execution_id:", execution_id)
    print("type(project_id):", type(project_id))
    print("project_id:", project_id)
    print("type(case_id):", type(case_id))
    print("case_id:", case_id)
    print("case_index:", case_index)
    print("callback_api:", callback_api)
    print("params_str:", params_str)
    print("type(params_str):", type(params_str))
    print("update_config_str:", update_config_str)

    update_config(update_config_str)

    if params_str:
        print("------- BaseTestCase.TestEngineCaseInput Before:")
        print(type(BaseTestCase.TestEngineCaseInput))
        print(BaseTestCase.TestEngineCaseInput)
        # todo: 此处需要优化一下 读取用户传递过来的 json 内容
        BaseTestCase.TestEngineCaseInput = json.loads(params_str.replace("'", '"'))
        print("------- BaseTestCase.TestEngineCaseInput After:")
        print(type(BaseTestCase.TestEngineCaseInput))
        print(BaseTestCase.TestEngineCaseInput)

    if execution_id:
        BaseTestCase.TestEngineCaseInput["execution_id"] = execution_id

    if project_id:
        BaseTestCase.TestEngineCaseInput["project_id"] = project_id

    if case_id:
        BaseTestCase.TestEngineCaseInput["case_id"] = case_id

    if case_index:
        BaseTestCase.TestEngineCaseInput["case_index"] = case_index

    if callback_api:
        BaseTestCase.TestEngineCaseInput["callback_api"] = callback_api

    testcase_list = []
    testcases = []

    # executed doctor() to check the necessary configuration files, auto generate them if not exists
    doctor.doctor()

    try:
        importlib.reload(settings)
    except BaseException:
        traceback.print_exc()

    args_init = args
    args = []

    if case_id:
        if case_id.find(",") > 0:
            for caseId in case_id.split(","):
                args.append(caseId)
        else:
            args = [case_id]
    else:
        if priority == None:
            #说明命令行中没有传入priority=XXX参数，则走判断 是否有 tag的流程
            # 如果case_id is None，则说明 命令行没有传入 case_id=XXX,则走原来没有集成 test-engine的流程
            for arg in args_init:
                if arg.find("=") == -1:
                    args.append(arg)
        else:
            #说明命令行中有传入priority=XXX参数，则走根据 priority 执行的流程
            args.append("tests") #此处直接把所有的tests加入参数中，后面再根据priority过滤

    print(" ======== final args:")
    print(args)

    if len(args) < 1:
        usage()
        return
    elif len(args) == 1 and (args[0].lower() == '--help' or args[0].lower() == '-help'):
        usage()
        return
    elif (args[0].lower() == '--help' or args[0].lower() == '-help' ) and (args[1].lower() == 'csv'):
        csv_usage()
        return

    elif len(args) >= 1:
        argument_deal_with_type_order = """
        for each parameter:
             first try to find it as a tag name,
             if no tag name found then try to find it as a TestcaseClassName,
             if no TestcaseClassName found then try to find it as a TestcaseList,
             if no TestcaseList found then try to find it as a package_name
        """
        print("the argument will be deal with by the follow order: %s" % argument_deal_with_type_order)
        try:
            auto_generate_testcase_list_from_csv.auto_generate_testcase_list_from_csv()
            testcase_dict_for_tags, testcase_dict_for_classname, testcase_dict_for_package = auto_generate_testcase_list.auto_generate_testcase_list()
            tags_list = []
            class_name_list = []
            package_list = []

            if project_id:
                # project_id 不为空 才执行如下操作
                for key in testcase_dict_for_tags.keys():
                    if len(key) > 0:
                        tags_list.append(key)
                for key in testcase_dict_for_classname.keys():
                    if len(key) > 0:
                        class_name_list.append(key)
                for key in testcase_dict_for_package.keys():
                    if len(key) > 0:
                        package_list.append(key)

                if len(package_list) > 0 or len(class_name_list) > 0 or len(tags_list) > 0:
                    print("package_list:")
                    print(package_list)
                    print("classNameList:")
                    print(class_name_list)
                    print("tagList:")
                    print(tags_list)

                    try:
                        headers = {
                            "Content-Type": "application/json",
                            "Host": "%s" % settings.TestEngineCallBackHost
                        }

                        callback_post_data = {
                            "id": "%s" % project_id,
                            "caseCount": len(testcase_dict_for_package["tests_testcases"]),
                            "packageList": package_list,
                            "classNameList": class_name_list,
                            "tagList": tags_list,
                        }

                        print("headers:")
                        print(headers)
                        print("callback_post_data:")
                        print(callback_post_data)

                        import requests
                        res = requests.post(
                            "%s/project/updateProjectCaseInfo" % settings.TestEngineCallBackAPI,
                            data=json.dumps(callback_post_data),
                            headers=headers)

                        print(res.json())
                    except BaseException:
                        traceback.print_exc()

            if case_id == 'auto_get_project_case_count_run_by_test_engine':
                print("-------=============-----------len(testcase_dict_for_package['tests_testcases']: %s" % len(testcase_dict_for_package["tests_testcases"]))
                print("will callback to test-engine with case_count & project_name")
                # todo: callback to test-engine with case_count & project_name, test-engine增加project时必须选择workflowId
                # 这样 传递 case_id=auto_get_project_case_count_run_by_test_engine & projectId=xxx的时候可以触发githubAction执行，此时需要同时
                # 传递 execution_id吗？ 注意 此 executoin_id是 d_project_case_count集合中的字段， 最好改成 fetch_id ?

                headers = {
                    "Content-Type": "application/json",
                    "Host": "%s" % settings.TestEngineCallBackHost
                }

                callback_post_data = {
                     # 此处 execution_id 其实是被执行的project的 projectId
                    # "projectId": "%s" % execution_id,
                    "projectId": "%s" % project_id,
                    "caseCount": len(testcase_dict_for_package["tests_testcases"]),
                }

                import requests
                res = requests.post(
                    "%s/projectCaseCount/create" % settings.TestEngineCallBackAPI,
                    data=json.dumps(callback_post_data),
                    headers=headers)


        except BaseException:
            traceback.print_exc()

        argument = args[0]

        if argument.lower() == "--help" or argument.lower() == "-help":
            usage()
            sys.exit()
        else:
            # the parameters for main.py can be any combination of the following 4 kinds which separated by a space:
            # 1. tag_name
            # 2. testcase's class_name
            # 3. testcase_list_name
            # 4. package_name
            #
            # e.g: execute CMD: "python main.py tag1 tag2 testcase_list1 testcase_list2 TestcaseClassName1 TestcaseClassName2 package_name_1"
            # all the testcase which fetched by the given parameters will be added into testcase_list, and will do
            # data deduplication for testcase_list
            #
            # for each parameter:
            #     first try to find it as a tag name, if no tag name found then try to find it as a TestcaseClassName,
            #     if no TestcaseClassName found then try to find it as a TestcaseList, if no TestcaseList found then
            #     try to find it as a package_name
            for argument in args:
                tag_name = None

                if argument == "quick_run":
                    continue  # todo: break? then quick_run can be the first argument, if break here, quick_run must be the last argument

                try:
                    if argument.__contains__("."):
                        print("Warning: there are '.' in tag {tag}, here replace '.' as '_' ".format(tag=argument))
                        tag_name = argument.replace(".", "_")
                    else:
                        tag_name = argument

                    # first try to find the argument as a tag name
                    if isinstance(testcase_dict_for_tags[tag_name], list):
                        for testcase_for_tag in testcase_dict_for_tags[tag_name]:
                            testcase_list.append(testcase_for_tag)

                    elif issubclass(testcase_dict_for_tags[tag_name], base_testcase.BaseTestCase):
                        testcase_list.append(testcase_dict_for_tags[tag_name])

                    print("******** Found TAG: %s" % tag_name)

                except KeyError:
                    # it no tag name found from testcases then try to search it as a TestcaseClassName
                    try:
                        found_in_classname = False
                        for tc in testcase_dict_for_classname.keys():
                            if tc.__contains__(argument):
                                testcase_list.append(testcase_dict_for_classname[tc])
                                found_in_classname = True
                                print("******** Found TestcaseClassName: %s" % tc)

                        if not found_in_classname:
                            raise KeyError("No TestcaseClassName Named '%s' Found!" % argument)
                    except KeyError:
                        # if no TestcaseClassName found from testcases then try to search it as a TestcaseList
                        try:
                            if isinstance(testcase_dict_for_package[argument], list):

                                for testcase_for_package in testcase_dict_for_package[argument]:
                                    testcase_list.append(testcase_for_package)

                            print("******** Found TestcaseList: %s" % argument)

                        except KeyError:
                            try:
                                if argument.__contains__("."):
                                    package_name = argument.replace(".", "_") + "_testcases"

                                    if isinstance(testcase_dict_for_package[package_name], list):

                                        for testcase_for_package in testcase_dict_for_package[package_name]:
                                            testcase_list.append(testcase_for_package)

                                    print("******** Found Package: %s" % argument)

                                elif argument == "tests":
                                    package_name = argument + "_testcases"

                                    if isinstance(testcase_dict_for_package[package_name], list):

                                        for testcase_for_package in testcase_dict_for_package[package_name]:
                                            testcase_list.append(testcase_for_package)

                                    print("******** Found Package: %s" % argument)
                                else:
                                    print('''
****** ERROR: NO Tag/TestcaseClassName/TestcaseList/Package Named '%s' Found in TestCases!

Tips:
Please review the following tips if you encounter this error:
  1. Ignored Tags: Check if the tag used in the testcase script includes the keyword "ignore".
  2. Multiple Entries: When executing multiple Tags, TestcaseClassNames, TestcaseLists, or Packages, ensure they are separated by spaces.
  3. Implementation Check: Verify that the run_test() function is implemented within the testcase script.
  4. DataSet/CSV Specifications: If you are utilizing the DataSet/CSV feature, confirm that it adheres to the specifications required by LinkTest.
     For more details, execute the command: python3 run.py --help csv.''' % argument)
                                    sys.exit()

                            except KeyError:
                                print('''
****** ERROR: NO Tag/TestcaseClassName/TestcaseList/Package Named '%s' Found in TestCases!

Tips:
Please review the following tips if you encounter this error:
  1. Ignored Tags: Check if the tag used in the testcase script includes the keyword "ignore".
  2. Multiple Entries: When executing multiple Tags, TestcaseClassNames, TestcaseLists, or Packages, ensure they are separated by spaces.
  3. Implementation Check: Verify that the run_test() function is implemented within the testcase script.
  4. DataSet/CSV Specifications: If you are utilizing the DataSet/CSV feature, confirm that it adheres to the specifications required by LinkTest.
     For more details, execute the command: python3 run.py --help csv.''' % argument)
                                sys.exit()

    if args[-1] == "quick_run":
        print("will execute by quick_run mode ...")

        testcase_module_class_list = []
        device_name_list = []

        testcase_dict_for_quick_run = {}

        for tc in testcase_list:
            tc_mddule_name = tc.__module__
            tc_class_name = tc.__name__

            if tc_class_name.__contains__("_real_device_"):
                testcase_module_class = tc_mddule_name.split("_real_device_")[0]
                testcase_module_class += "." + tc_class_name.split("_real_device_")[0]
                testcase_device_name = "real_device_" + tc_class_name.split("_real_device_")[1]

                if testcase_module_class not in testcase_module_class_list:
                    testcase_module_class_list.append(testcase_module_class)

                if testcase_device_name not in device_name_list:
                    device_name_list.append(testcase_device_name)

                testcase_dict_for_quick_run[tc_mddule_name + "." + tc_class_name] = tc

            elif tc_class_name.__contains__("_virtual_device_"):
                testcase_module_class = tc_mddule_name.split("_virtual_device_")[0]
                testcase_module_class += "." + tc_class_name.split("_virtual_device_")[0]
                testcase_device_name = "virtual_device_" + tc_class_name.split("_virtual_device_")[1]

                if testcase_module_class not in testcase_module_class_list:
                    testcase_module_class_list.append(testcase_module_class)

                if testcase_device_name not in device_name_list:
                    device_name_list.append(testcase_device_name)

                testcase_dict_for_quick_run[tc_mddule_name + "." + tc_class_name] = tc

        quick_run_list = []
        # here begin to get the quick_run_list by testcase_module_class_list & device_name_list
        index = 0
        for module_class_str in testcase_module_class_list:
            if index < len(device_name_list):
                device_name = device_name_list[index]

                module_name_str, class_name_str = functools.reduce(lambda x, y: x + "." + y,
                                                                   module_class_str.split(".")[:-2]) + "." + \
                                                  module_class_str.split(".")[-2] + "_" + device_name, \
                                                  module_class_str.split(".")[-1] + "_" + device_name

                quick_run_list.append(module_name_str + "." + class_name_str)

                index += 1
                if index == len(device_name_list):
                    index = 0

        # here begin to tran testcase_list to a dict
        quick_run_testcase_list = []
        for t in quick_run_list:
            quick_run_testcase_list.append(testcase_dict_for_quick_run[t])

        for tc in testcase_list:
            if not issubclass(tc, android_testcase.AndroidTestCase):
                quick_run_testcase_list.append(tc)

        testcase_list = quick_run_testcase_list
        # todo:  auto generate quick_run_testcase_list string list under tag_packages/ ?

    if isinstance(testcase_list, list):
        for testcase in testcase_list:
            if testcase not in testcases:
                testcases.append(testcase)

    elif issubclass(testcase_list, base_testcase.BaseTestCase):
        testcases.append(testcase_list)

    if testcases.__len__() == 0:
        print("************************ there are no testcases will be executed ************************")
        usage()
    else:
        # clean the execution environment
        if not settings.DEBUG_RUN:
            close_browsers_and_webdrivers()
        else:
            clean_appium_env()

        output_folder = project_info.output_folder
        begin_time = datetime.datetime.now()
        print("this execution's logs & screenshots under output_folder: %s" % output_folder)
        # QUEUE_SIZE is used to set how many test cases will be executed concurrently in settings.__init__.py
        # if not found, here set its default value as 1
        thread_pool_size = getattr(settings, "QUEUE_SIZE", 1)

        queue = collections.deque()

        include_android_testcase_flag = False
        non_backend_testcase_count = 0

        if priority:
            # 如果命令行参数中设置了 priority
            # fetch testcases and add into queue
            for testcase in testcases:
                if not issubclass(testcase, api_testcase.APITestCase):
                    non_backend_testcase_count += 1

                if issubclass(testcase, android_testcase.AndroidTestCase):
                    include_android_testcase_flag = True
                    device_name = TestCaseExecutor.get_device_name(testcase)

                    exec("TestCaseExecutor.%s = False" % device_name)

                # 只有大于或等于制定优先级的testcase才会被加入中执行队列中
                if testcase.priority is not None and testcase.priority >= priority:
                    print("add testcase:%s  ----  with priority:%s" % (testcase, testcase.priority))
                    queue.append(testcase)
        else:
            # fetch testcases and add into queue
            for testcase in testcases:
                if not issubclass(testcase, api_testcase.APITestCase):
                    non_backend_testcase_count += 1

                if issubclass(testcase, android_testcase.AndroidTestCase):
                    include_android_testcase_flag = True
                    device_name = TestCaseExecutor.get_device_name(testcase)

                    exec("TestCaseExecutor.%s = False" % device_name)

                queue.append(testcase)

        total_testcases_count = len(queue)
        testcase_executor_list = []

        if total_testcases_count == 0:
            print("\n\n\n++++++++++++++++++++++++++++ No testcases will be executed! ++++++++++++++++++++++++++++\n"
                                     "Please execute CMD 'python3 run.py --help' for more info :) ")
            return

        if non_backend_testcase_count > 0:
            import multiprocessing
            if thread_pool_size > multiprocessing.cpu_count() * 2:
                thread_pool_size = multiprocessing.cpu_count() * 2
                print("So many Non_Backend testcases concurrently run, here set the thread_pool_size = (2 * cpu_count:%s = %s)" % (
                        multiprocessing.cpu_count(), 2 * multiprocessing.cpu_count()))

        if total_testcases_count < thread_pool_size:
            thread_pool_size = total_testcases_count

        if include_android_testcase_flag is True:
            thread_pool_size = len(get_adb_devices())

        if thread_pool_size == 1:
            print("***" * 20 + " will start %s thread" % thread_pool_size)
        else:
            print("***" * 20 + " will start %s threads" % thread_pool_size)
        for i in range(thread_pool_size):
            testcase_executor_list.append(TestCaseExecutor(queue, total_testcases_count, output_folder, begin_time))

        BaseTestCase.GlobalTotalCaseCount = total_testcases_count

        for testcase_executor in testcase_executor_list:
            time.sleep(0.1)
            testcase_executor.start()

        for testcase_executor in testcase_executor_list:
            testcase_executor.join()

        final_failed_testcase_list = []
        if len(TestCaseExecutor.failed_testcases) > 0:
            for tc in TestCaseExecutor.failed_testcases:
                final_failed_testcase_list.append(max(set(tc.packages.split(",")), key=len) + os.sep + tc.__class__.__name__)
                
            # remove duplicate testcases in failed_testcase_name_list （rerun_flag=True时，会有重复的testcase）
            final_failed_testcase_list = list(set(final_failed_testcase_list))

        print(os.linesep + "****************** TestCases Passed: %s, TestCases Failed: %s, Total TestCases Executed: %s ******************" % (
                len(TestCaseExecutor.passed_testcases),len(final_failed_testcase_list), total_testcases_count))

        if len(final_failed_testcase_list) > 0:
            print("Failed TestCase List:")
            print(final_failed_testcase_list)

        if not settings.DEBUG_RUN:
            close_browsers_and_webdrivers()
        else:
            clean_appium_env()


        try:
            if getattr(settings, "generate_xunit_result", False):
                xml_report.generate_xunit_result(TestCaseExecutor.passed_testcases,
                                                 TestCaseExecutor.failed_testcases,
                                                 output_folder,
                                                 begin_time
                                                 )
                print("generate xml report done...")
        except BaseException:
            traceback.print_exc()
            with open(output_folder + "error_log.txt", "w") as file_obj:
                file_obj.write(traceback.format_exc())

        try:
            rerun_flag = getattr(settings, "RERUN_FLAG", False)

            if getattr(settings, "SAVE_LOG_TO_DB", False):
                from . import database_helper

                database_helper.save_execution_log(passed_testcases=TestCaseExecutor.passed_testcases,
                                                   failed_testcases=TestCaseExecutor.failed_testcases,
                                                   output_folder=output_folder,
                                                   project_name=project_info.project_name,
                                                   jenkins_job_name=TestCaseExecutor.jenkins_job_name,
                                                   rerun_flag=rerun_flag)
        except BaseException:
            traceback.print_exc()

        try:
            html_report.Reporter(output_folder,
                                 TestCaseExecutor.passed_testcases,
                                 TestCaseExecutor.failed_testcases,
                                 TestCaseExecutor.error_testcases,
                                 begin_time,
                                 PLATFORM_INFO,
                                 len(BaseTestCase.GlobalDataList) > 0,
                                 rerun_flag=rerun_flag)
        except BaseException:
            traceback.print_exc()

        print(
            "=*=*=" * 20 + os.linesep + "Execution has been completed. For further details, please refer to the accompanying HTML report:" + os.linesep + "file:///" + output_folder + os.sep + "report.html" + os.linesep)

        # save the execution summary into DB (environment, total_execution_time,
        try:
            import linktest
            from . import database_helper

            if getattr(settings, "SAVE_LOG_TO_DB", False):
                end_time = datetime.datetime.now()
                execution_time = end_time - begin_time

                database_helper.insert_execution_summary_log(
                    execution_id=output_folder.split(os.sep + "output" + os.sep)[1],
                    environment=settings.ENVIRONMENT,
                    os=PLATFORM_INFO,
                    automation_framework_version=linktest.__version__,
                    total_execution_time=execution_time,
                    jenkins_job_name=None,
                    project_name=project_info.project_name,
                    total_testcases_count=total_testcases_count,
                    pass_testcases_count=len(TestCaseExecutor.passed_testcases),
                    fail_testcases_count=total_testcases_count - len(TestCaseExecutor.passed_testcases),
                    rerun_flag=1 if rerun_flag else 0
                )
        except BaseException:
            traceback.print_exc()
        

        # 如果最后执行完所有的testcase 并且GlobalDataList 不为空，则自动保存GlobalDataList
        if len(BaseTestCase.GlobalDataList) > 0:
            file_path = os.path.join(output_folder, "global_data_list.py")
            try:
                with open(file_path, "w", encoding='utf-8') as f:
                    f.write("global_data_list = " + json.dumps(BaseTestCase.GlobalDataList, indent=4, ensure_ascii=False))
            except OSError as e:
                print("Error writing global_data_list to file:")
                print(str(e))
            except Exception as e:
                print("An unexpected error occurred:")
                print(str(e))

        return output_folder, BaseTestCase.GlobalExecutedCaseList[-1].logfile_full_name


if __name__ == "__main__":
    close_browsers_and_webdrivers()
    run_with(sys.argv)

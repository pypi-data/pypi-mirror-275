""" Fine Tuner of Triplet generator using Rebel model
    Usage:

        alias pyrebel="python3 -u rag/rebel_train.py "

        # prepare fine-tuning dataset
            pyrebel data_norm_fakelabel --dirin 'ztmp/bench/norm/ag_news/train/df_1.parquet' --dirout 'ztmp/kg/train/ft_data.csv'
        

        # train
            pyrebel  run_train --dirtrain 'ztmp/kg/train/ft_data.parquet' --dirout "./ztmp/out/rebel_finetune"



    Dataset Label Sample:
        https://huggingface.co/Babelscape/rebel-large

        text: "European Union Extends Microsoft-Time Warner Review BRUSSELS, Belgium (AP) -- European antitrust regulators said Monday they have extended their review of a deal between Microsoft Corp. (MSFT) and Time Warner Inc...",
        label: <s><triplet> European Union Extends Microsoft-Time Warner Review <subj> antitrust regulators <obj> instance of <triplet> BRUSSELS <subj> Belgium <obj> country <triplet> Belgium <subj> BRUSSELS <obj> capital</s>
        <s> =>start,
        </s> => end,
        <triplet> => start of triplet,
        <subj> => denotes preceeding string to be subject,
        <obj> => denotes preceeding string to be object

        text: "Patriots Sign Top Pick Watson (Reuters) Reuters - The New England Patriots\Monday announced the signing of first-round draft pick Benjamin\Watson. As per team policy, terms of the tight end's deal were\not released."
        label: <s><triplet> Benjamin\Watson <subj> New England Patriots <obj> member of sports team</s>


    Findings:
                "This is a multilingual version of REBEL(rebel-large). It can be used as a standalone multulingual Relation Extraction
                system, or as a pretrained system to be tuned on multilingual Relation Extraction datasets."
                - mrebel-base hf page

                If we are to stick to English for now, I think we should train, finetune rebel-large.
                model size:
                mrebel-base: 484M params
                rebel-large: 406M params
                So rebel-large is smaller than mrebel-base.


    Infos:
        https://arxiv.org/pdf/2310.00696


"""
import evaluate
import fire
import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.metrics._scorer import metric
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer
from utilmy.webapi.asearch.rag.engine import torch_getdevice

from utilmy import pd_read_file, pd_to_file


#### Global for Jupyter/
def model_init(model_name="Babelscape/rebel-large"):
    global device, tokenizer, model
    device = torch_getdevice()
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    model.to(device)

    metric = evaluate.load('accuracy')

    return model, tokenizer, metric


################################################################################################
def data_norm_fakelabel(dirin='ztmp/bench/norm/ag_news/train/df_1.parquet',
                        dirout="ztmp/kg/train/ft_data.parquet",
                        model_name="Babelscape/rebel-large",
                        batch_size=8, nrows=-1):
    """Generate a fine-tuning dataset for training a language model.

        python3 -u rag/rebel_train.py  run_train --dirin 'ztmp/kg/train/ft_data.parquet' --dirout "./ztmp/out/rebel_finetune"

        Args:
            dirin (str):  input  path containing  dataset in parquet format.
            dirout (str):  output  path to save  fine-tuning dataset in CSV format.
            batch_size (int):  number of rows to process in each batch. Defaults to 8.
            nrows (int):  number of rows to process. If -1, processes all rows.

        Returns:
            None

        This function reads a dataset from  input directory, processes it in batches,
        generates triplets based on  text using  rebel-base, and saves the
        resulting fine-tuning dataset in  output directory.  dataset consists of
        two columns: 'text' and 'labels'.  'text' column contains  input text,
        and  'labels' column contains  output text generated by rebel-base.

        Example usage:
            data_norm_fakelabel(dirin='path/to/input/dataset.parquet', dirout='path/to/output/dataset.csv')
    """
    result_df = pd.DataFrame(columns=["text", "labels"])

    model, tokenizer, metric = model_init(model_name=model_name)

    df = pd_read_file(dirin)
    if nrows == -1:
        nrows = len(df)
    # result_df = df[:nrows]
    # generate triplets based on text using rebel-base
    ft_text = []
    ft_output = []
    for i in range(0, nrows, batch_size):
        df_subset = df[i:i + batch_size]
        text = df_subset['body'].tolist()
        ft_text.extend(text)
        output_text = rebel_generate_tripletlabel(model, tokenizer, text)
        ft_output.extend(output_text)

    result_df["text"] = ft_text
    result_df["labels"] = ft_output
    pd_to_file(result_df, dirout)


def run_train(dirtrain="ztmp/kg/train/ft_data.parquet",
              dirval=None,
              dirout='./ztmp/out/rebel_finetune/', nrows=-1):
    model, tokenizer, metric = model_init(model_name="Babelscape/rebel-large")

    ds_train = data_tokenize_split(dirtrain, tokenizer, nrows=nrows)

    if isinstance(dirval, str):
        ds_val = data_tokenize_split(dirval, nrows=nrows)
    else:
        # split dataset into train and test
        ds = ds_train.train_test_split(test_size=0.1)
        ds_train = ds["train"]
        ds_val = ds["test"]

    training_args = TrainingArguments(output_dir=dirout)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds_train,
        eval_dataset=ds_val
    )
    trainer.train()
    trainer.save_model(f"{dirout}/model")


####################################################################################################
def data_tokenize_split(dirin='ztmp/kg/train/ft_data.parquet', tokenizer=None, nrows=10):
    """ 
    dataset_hf = raw_dataset.map(lambda x:
                                 {'input_ids': tokenizer(x['text'], truncation=True, padding=True)[
                                     "input_ids"],
                                  "attention_mask":
                                      tokenizer(x["text"], truncation=True, padding=True)[
                                          "attention_mask"],
                                  'labels': tokenizer(x["labels"], truncation=True, padding=True)[
                                      "input_ids"]}
                                 )


    # map input_ids and attention mask via single call per text
    # train_data = ds.map(
    #     lambda x: {k: v for k, v in tokenizer(x['text'], truncation=True, padding=True, max_length=max_length).items()
    #                if k in ['input_ids', 'attention_mask']},
    #     batched=True, remove_columns=['text'])

    """
    if tokenizer is None:
        _, tokenizer, _ = model_init(model_name="Babelscape/rebel-large")

    df = pd_read_file(dirin)
    ###  text: 
    ###  labels:  "__en__ __sv__ BRUSSELS       __vi__ Belgium        __tn__ country</s>"
    df = df[["text", "labels"]]

    df = df[:nrows]
    ds = Dataset.from_pandas(df)
    # raw_dataset = raw_dataset.train_test_split(test_size=0.1)
    max_length = 128

    # def zzz_preprocess_func(row):
    #     #### Text tokenizer
    #     row_ddict = tokenizer(row["text"], truncation=True, padding=True, max_length=max_length,
    #                           return_offsets_mapping=False,
    #                           return_overflowing_tokens=False)
    #
    #     # out["labels"] = row["labels"]
    #     # log(out)
    #     # output["input_ids"] = output.pop("input_ids")  # Add input_ids to  output
    #     return row_ddict

    # ds = ds.map(preprocess_func, batched=True, remove_columns=['text', 'labels'])

    def prepro_text(x):
      return {k: v for k, v in tokenizer(x['text'], truncation=True, padding=True, max_length=max_length).items()
                   if k in ['input_ids', 'attention_mask']}

    train_data = ds.map(lambda x:  prepro_text(x), batched=True, remove_columns=['text'])



    # set labels to their input_ids
    def prepro_label(x):
      return {"labels": v for k, v in tokenizer(x['labels'], truncation=True, padding=True).items() if
                   k in ['input_ids']}

    train_data = train_data.map(lambda x: prepro_label(x),batched=True)
    return train_data


##########################################################################################
# def tokenize_function(examples):
#     return tokenizer(examples['text'], padding='max_length', truncation=True)


def rebel_generate_tripletlabel(model, tokenizer, text: list) -> list:
    """ 
        Label format : 

    """
    inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt").to(model.device)
    outputs = model.generate(inputs['input_ids'])
    result = tokenizer.batch_decode(outputs, skip_special_tokens=False)
    result = [res.replace("<pad>", "") for res in result]
    # print(result)
    return result


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


if __name__ == '__main__':
    fire.Fire()

""" Logs output:
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map: 100%|██████████████████████████████████████████████| 7/7 [00:00<00:00, 1606.31 examples/s]
Map: 100%|██████████████████████████████████████████████| 7/7 [00:00<00:00, 1967.31 examples/s]
{'train_runtime': 2.3199, 'train_samples_per_second': 7.759, 'train_steps_per_second': 1.293, 'train_loss': 7.620423634847005, 'epoch': 3.0}
100%|████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.29it/s]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 200, 'num_beams': 5}

"""

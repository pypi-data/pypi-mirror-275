---
jupyter:
  celltoolbar: Create Assignment
  jupytext:
    cell_metadata_json: true
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  otter:
    OK_FORMAT: true
    tests:
      q1a:
        name: q1a
        points: 1
        suites:
        - cases:
          - code: '>>> is_bid_unique or ~is_bid_unique

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q1b:
        name: q1b
        points: 2
        suites:
        - cases:
          - code: '>>> assert len(top_names) == 5

              >>> assert len(top_addresses) == 5

              '
            hidden: false
            locked: false
          - code: '>>> assert top_names[-1] == "Proper Food" or top_names[-1] == "STARBUCKS"

              >>> assert top_addresses[-1] == "Pier 41"

              '
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q1c:
        name: q1c
        points: 1
        suites:
        - cases:
          - code: '>>> q1c.upper() in set(["A", "B", "C"])

              True'
            hidden: false
            locked: false
            points: 0
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q2a:
        name: q2a
        points: 1
        suites:
        - cases:
          - code: '>>> type(zip_counts) == pd.Series

              True'
            hidden: false
            locked: false
          - code: '>>> zip_counts.shape[0] == 63

              True'
            hidden: false
            locked: false
          - code: '>>> zip_counts["94103"] == 562

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q2bi:
        name: q2bi
        points: 1
        suites:
        - cases:
          - code: '>>> type(valid_zip_numbers) == pd.Series

              True'
            hidden: false
            locked: false
          - code: '>>> valid_zip_numbers.dtype == np.dtype(np.int64)

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q2bii:
        name: q2bii
        points: 1
        suites:
        - cases:
          - code: ">>> assert type(invalid_zip_bus) == pd.DataFrame \n>>> len(invalid_zip_bus)\
              \ == 230\nTrue"
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q2ci:
        name: q2ci
        points: 1
        suites:
        - cases:
          - code: '>>> assert type(missing_postal_code) == str

              '
            hidden: false
            locked: false
          - code: '>>> assert bus_missing.shape[0] == 194

              '
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q2cii:
        name: q2cii
        points: 1
        suites:
        - cases:
          - code: '>>> type(missing_zip_address_count) == pd.Series

              True'
            hidden: false
            locked: false
          - code: '>>> assert len(missing_zip_address_count) == 135

              >>> missing_zip_address_count[''3914 Judah St''] == 1

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q2e:
        name: q2e
        points: 2
        suites:
        - cases:
          - code: '>>> assert ''postal5'' in bus.columns

              >>> (bus[''postal5''].str.len() != 5).sum() == 221

              True'
            hidden: false
            locked: false
          - code: '>>> assert bus[''postal5''].isin(valid_zips).sum() == 6032

              >>> bus[''postal5''].isna().sum() == 221

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q3a:
        name: q3a
        points: 1
        suites:
        - cases:
          - code: '>>> type(is_ins_iid_unique) == bool or type(is_ins_iid_unique)
              == np.bool_

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q3b:
        name: q3b
        points: 2
        suites:
        - cases:
          - code: '>>> ''bid'' in ins.columns

              True'
            hidden: false
            locked: false
          - code: '>>> ins[''bid''].dtype == int

              True'
            hidden: false
            locked: false
          - code: '>>> len(ins[ins[''score''] > 0][''bid''].unique()) == 5724

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q3ci:
        name: q3ci
        points: 1
        suites:
        - cases:
          - code: '>>> type(ins_date_type) == type

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q3cii:
        name: q3cii
        points: 1
        suites:
        - cases:
          - code: '>>> type(ins[''timestamp''][1]) == pd.Timestamp

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q3ciii:
        name: q3ciii
        points: 1
        suites:
        - cases:
          - code: '>>> type(earliest_date) == pd.Timestamp

              True'
            hidden: false
            locked: false
          - code: '>>> type(latest_date) == pd.Timestamp

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q3civ:
        name: q3civ
        points: 1
        suites:
        - cases:
          - code: '>>> ''year'' in ins.columns

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q4a:
        name: q4a
        points: 1
        suites:
        - cases:
          - code: '>>> len(bus.columns) == 11 and ''name_length'' in bus.columns

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q4b:
        name: q4b
        points: 1
        suites:
        - cases:
          - code: '>>> bus_valid.shape[0] == 6032

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q4c:
        name: q4c
        points: 1
        suites:
        - cases:
          - code: '>>> np.all((type(is_even) == pd.Series) and is_even.unique() ==
              [True, False])

              True'
            hidden: false
            locked: false
          - code: '>>> len(is_even) == len(bus_valid)

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q4d:
        name: q4d
        points: 2
        suites:
        - cases:
          - code: '>>> type(longest_name_even) == str

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
      q4e:
        name: q4e
        points: 1
        suites:
        - cases:
          - code: '>>> same_longest_name in [True, False]

              True'
            hidden: false
            locked: false
          scored: true
          setup: ''
          teardown: ''
          type: doctest
---

```{python deletable=FALSE, editable=FALSE}
# Initialize Otter
import otter
grader = otter.Notebook("pandas_exploration.ipynb")
```

<!-- #region {"nbgrader": {"grade": false, "grade_id": "intro-hw2", "locked": true, "schema_version": 2, "solution": false}} -->
#  Food Safety

This notebook is practice for cleaning and exploring data with Pandas.

## This Assignment

In this assignment, we will investigate restaurant food safety scores for
restaurants in San Francisco. The scores and violation information have been
[made available by the San Francisco Department of Public
Health](https://data.sfgov.org/Health-and-Social-Services/Restaurant-Scores-LIVES-Standard/pyih-qa8i).
The main goal for this assignment is to walk through the process of Data
Cleaning and familiarize yourself with some of the Pandas functions we have
gone through in the classes.

After this homework, you should be comfortable with:
* Reading CSV files, 
* Reading `Pandas` documentation and using `Pandas`,
* Working with data at different levels of granularity,
* Identifying the type of data collected, missing values, anomalies, etc., and
  doing some basic analysis

## Score Breakdown 

Question | Manual | Points
--- | --- | ---
1a | no | 1
1b | no | 2
1c | no | 1
2a | no | 1
2bi | no | 1
2bii | no | 1
2ci | no | 1
2cii | no | 1
2d | yes | 2
2e | no | 2
3a | no | 1
3b | no | 2
3ci | no | 1
3cii | no | 1
3ciii | no | 1
3civ | no | 1
4a | no | 1
4b | no | 1
4c | no | 1
4d | no | 2
4e | no | 1
Total | 1 | 24

<!-- #endregion -->

## Before You Start

For each question in the assignment, please write down your answer in the
answer cell(s) right below the question. 

**Important note: The local autograder tests in this notebook will not be comprehensive.
You can pass the automated tests in your notebook but still fail tests in the
autograder used for marking the assessment.** Please be sure to check your
results carefully.

Finally, unless we state otherwise, **do not use for loops or list
comprehensions**. The majority of this assignment can be done using built-in
commands in `Pandas` and `Numpy`.  You're depriving yourself of key learning
objectives if you write loops / comprehensions.

The cell below imports all the necessary libraries you need to use during this
assignment. Without running this cell, you will not be able to call the various
Numpy and Pandas functions we use later on, so please make sure you run it
before starting to work on the assignment.

```{python nbgrader={'grade': False, 'grade_id': 'import', 'locked': True, 'schema_version': 2, 'solution': False}}
import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
plt.style.use('fivethirtyeight')

from IPython.display import display, Image 
def display_figure_for_grader(fig):
    plotly.io.write_image(fig, 'temp.png')
    display(Image('temp.png'))    
```

<hr style="border: 5px solid #003262;" />
<hr style="border: 1px solid #fdb515;" />

<!-- #region -->
# 0: Obtaining the Data

## File Systems and I/O

<!-- #endregion -->

In general, we will focus on using Python commands to investigate files.  However, it can sometimes be easier to use shell commands in your local operating system.  The following cells demonstrate how to do this.

```{python}
from pathlib import Path
data_dir = Path('.')
data_dir.mkdir(exist_ok = True)
file_path = data_dir / Path('data.zip')
dest_path = file_path
```

After running the cell above, if you list the contents of the directory containing this notebook, you should see `data.zip`.

*Note*: The command below starts with an `!`. This tells our Jupyter Notebook to pass this command to the operating system. In this case, the command is the `ls` Unix command which lists files in the current directory.

```{python}
# !ls
```

## Loading Food Safety Data

We have data, but we don't have any specific questions about the data yet. Let's focus on understanding the structure of the data; this involves answering questions such as:

* Is the data in a standard format or encoding?
* Is the data organized in records?
* What are the fields in each record? (We sometimes also use the term 'feature' or 'attribute' as well, depending on the context)

Let's start by looking at the contents of `data.zip`. It's not just a single file but rather a compressed directory of multiple files. We could inspect it by uncompressing it using a shell command such as `!unzip data.zip`, but in this homework we're going to do almost everything in Python for maximum portability.

## Looking Inside and Extracting the Zip Files

The following code blocks are for setup. Simply run the cells; **do not modify them**. Question 1a is where you will start to write code.

Here, we assign `my_zip` to a `zipfile.Zipfile` object representing `data.zip`, and assign `list_names` to a list of all the names of the contents in `data.zip`.

```{python}
import zipfile
my_zip = zipfile.ZipFile(dest_path, 'r')
list_names = my_zip.namelist()
list_names
```

You may notice that we did not write `zipfile.ZipFile('data.zip', ...)`. Instead, we used `zipfile.ZipFile(dest_path, ...)`. In general, we **strongly suggest having your filenames hard coded as string literals only once** in a notebook. It is very dangerous to hardcode things twice because if you change one but forget to change the other, you can end up with bugs that are very hard to find.


Now, we display the files' names and their sizes.

```{python}
my_zip = zipfile.ZipFile(dest_path, 'r')
for info in my_zip.infolist():
    print('{}\t{}'.format(info.filename, info.file_size))
```

Often when working with zipped data, we'll never unzip the actual zipfile. This saves space on our local computer. However, for this homework the files are small, so we're just going to unzip everything. This has the added benefit that you can look inside the CSV files using a text editor, which might be handy for understanding the structure of the files. The cell below will unzip the CSV files into a sub-directory called `data`.

```{python}
data_dir = Path('.')
my_zip.extractall(data_dir)
# !ls {data_dir / Path("data")}
```

The cell above created a folder called `data`, and in it there should be five
CSV files. Let's open up `legend.csv` to see its contents. To do this, click on
the file icon on the top left to show the folders and files within the folder
containing this notebook, then click on `legend.csv`. The file will open up in
another tab. You should see something that looks like:

    "Minimum_Score","Maximum_Score","Description"
    0,70,"Poor"
    71,85,"Needs Improvement"
    86,90,"Adequate"
    91,100,"Good"


The `legend.csv` file does indeed look like a well-formed CSV file. Let's check the other three files. Rather than opening up each file manually, let's use Python to print out the first 5 lines of each. We defined a helper function for you that will allow you to retrieve the first N lines of a file as a list. For example, `head('data/legend.csv', 5)` will return the first 5 lines of "data/legend.csv". Run the cell below to print out the first 5 lines of all six files that we just extracted from the zipfile.

```{python}
import os

def head(filename, lines=5):
    """
    Returns the first few lines of a file.
    
    filename: the name of the file to open
    lines: the number of lines to include
    
    return: A list of the first few lines from the file.
    """
    return Path(filename).read_text().splitlines()[:lines]


data_dir = "./"
for f in list_names:
    if not os.path.isdir(f):
        print(head(data_dir + f, 5), "\n")
```

## Reading in and Verifying Data

Based on the above information, let's attempt to load `bus.csv`, `ins2vio.csv`, `ins.csv`, and `vio.csv` into Pandas Dataframes with the following names: `bus`, `ins2vio`, `ins`, and `vio`, respectively.

*Note:* Because of character encoding issues, one of the files (`bus`) will require an additional argument `encoding='ISO-8859-1'` when calling `pd.read_csv`. At some point in your future, you should read all about [character encodings](https://diveintopython3.problemsolving.io/strings.html). We won't discuss these in detail in the course.

```{python}
# Path to the directory containing data
dsDir = Path('data')

bus = pd.read_csv(dsDir/'bus.csv', encoding='ISO-8859-1')
ins2vio = pd.read_csv(dsDir/'ins2vio.csv')
ins = pd.read_csv(dsDir/'ins.csv')
vio = pd.read_csv(dsDir/'vio.csv')

# This code is essential for the autograder to function properly. Do not edit
ins_test = ins
```

Now that you've read the files, let's try some `pd.DataFrame` methods ([docs](https://pandas.pydata.org/pandas-docs/version/1.4.3/reference/api/pandas.DataFrame.html)).
Use the `DataFrame.head` method to show the top few lines of the `bus`, `ins`, and `vio` Dataframes. For example, running the cell below will display the first few lines of the `bus` Dataframe. 

```{python}
bus.head()
```

To show multiple return outputs in one single cell, you can use `display()`. 

```{python}
display(bus.head())
display(ins.head())
```

The `DataFrame.describe` method can also be handy for computing summaries of numeric columns of our Dataframes. Try it out with each of our 4 Dataframes. Below, we have used the method to give a summary of the `bus` Dataframe. 

```{python}
bus.describe()
```

Now, we perform some sanity checks for you to verify that the data was loaded with the correct structure.


First, we check the basic structure of the data frames you created (these tests have passed if the cell runs without errors):

```{python}
assert all(bus.columns == ['business id column', 'name', 'address', 'city', 'state', 'postal_code',
                           'latitude', 'longitude', 'phone_number'])
assert 6250 <= len(bus) <= 6260

assert all(ins.columns == ['iid', 'date', 'score', 'type'])
assert 26660 <= len(ins) <= 26670

assert all(vio.columns == ['description', 'risk_category', 'vid'])
assert 60 <= len(vio) <= 65

assert all(ins2vio.columns == ['iid', 'vid'])
assert 40210 <= len(ins2vio) <= 40220
```

Next we'll check that the statistics match what we expect. The following are hard-coded statistical summaries of the correct data.

```{python}
bus_summary = pd.DataFrame(**{'columns': ['business id column', 'latitude', 'longitude'],
 'data': {'business id column': {'50%': 75685.0, 'max': 102705.0, 'min': 19.0},
  'latitude': {'50%': -9999.0, 'max': 37.824494, 'min': -9999.0},
  'longitude': {'50%': -9999.0,
   'max': 0.0,
   'min': -9999.0}},
 'index': ['min', '50%', 'max']})

ins_summary = pd.DataFrame(**{'columns': ['score'],
 'data': {'score': {'50%': 76.0, 'max': 100.0, 'min': -1.0}},
 'index': ['min', '50%', 'max']})

vio_summary = pd.DataFrame(**{'columns': ['vid'],
 'data': {'vid': {'50%': 103135.0, 'max': 103177.0, 'min': 103102.0}},
 'index': ['min', '50%', 'max']})

from IPython.display import display

print('What we expect from your Businesses Dataframe:')
display(bus_summary)
print('What we expect from your Inspections Dataframe:')
display(ins_summary)
print('What we expect from your Violations Dataframe:')
display(vio_summary)
```

The code below defines a testing function that we'll use to verify that your
data has the same statistics as what we expect. Run these cells to define the
function. The `df_allclose` function has this name because we are verifying
that all of the statistics for your Dataframe are close to the expected values.
Why not `df_allequal`? It's a bad idea in almost all cases to compare two
floating point values like 37.780435, as rounding errors can cause spurious
failures (e.g. the test will fail because the numbers are different by a tiny
amount, because of the large but limited precision of numbers represented by
the compute, not because they are the wrong numbers). Run the following cells
to load some basic utilities (you do not need to change these at all):

```{python}
"""Run this cell to load this utility comparison function that we will use in various
tests below (both tests you can see and those we run internally for grading).

Do not modify the function in any way.
"""


def df_allclose(actual, desired, columns=None, rtol=5e-2):
    """Compare selected columns of two Dataframes on a few summary statistics.

    Compute the min, median and max of the two Dataframes on the given columns, and compare
    that they match numerically to the given relative tolerance.
    
    If they don't match, an AssertionError is raised (by `numpy.testing`).
    """    
    # Summary statistics to compare on
    stats = ['min', '50%', 'max']
    
    # For the desired values, we can provide a full DF with the same structure as
    # the actual data, or pre-computed summary statistics.
    # We assume a pre-computed summary was provided if column is None. In that case, 
    # `desired` *must* have the same structure as the actual's summary
    if columns is None:
        des = desired
        columns = desired.columns
    else:
        des = desired[columns].describe().loc[stats]

    # Extract summary stats from actual DF
    act = actual[columns].describe().loc[stats]

    return np.allclose(act, des, rtol)
```

We will now explore each file in turn, including determining its granularity and exploring many of the variables individually. Let's begin with the businesses file, which has been read into the `bus` Dataframe.

<!-- #region {"nbgrader": {"grade": false, "grade_id": "business-data", "locked": true, "schema_version": 2, "solution": false}} -->
<br/><br/><br/>

---

<br/><br/><br/>

# Question 1: Examining the Business Data File

## Question 1a
<!-- #endregion -->

From its name alone, we expect the `bus.csv` file to contain information about the restaurants. Let's investigate the granularity of this dataset.

```{python}
bus.head()
```

The `bus` Dataframe contains a column called `business id column` which probably corresponds to a unique business id.  However, we will first rename that column to `bid` for simplicity.

**Note**: In practice, we might want to do this renaming when the table is loaded but for grading purposes we will do it here.


```{python}
bus = bus.rename(columns={"business id column": "bid"})
```

<!-- #region {"deletable": false, "editable": false} -->
Examining the entries in `bus`, is the `bid` unique for each record (i.e. each row of data)? Your code should compute the answer, i.e. don't just hard code `True` or `False`.

**Hint**: Use [`value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) or [`unique()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) to determine if the `bid` series has any duplicates.
<!-- #endregion -->

```{python}
is_bid_unique = bus['bid'].value_counts().max() == 1
# Show the result.
is_bid_unique
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q1a")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/><br/>

---


## Question 1b

We will now work with some important fields in `bus`.

1. Assign `top_names` to an sequence (a `list` will do, or an array) containing
   the top 5 most frequently used business names, from most frequent to least
   frequent.
2. Assign `top_addresses` to a sequence containing the top 5 addresses where
   businesses are located, from most popular to least popular.

**Hint 1**: You may find
[value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)
helpful.

**Hint 2**: You'll need to get the names / addresses, NOT the counts associated
with each.  It may help to remember that the row labels for Pandas' Series are
in the `.index`. If you're unsure how to do this, try looking through the
textbooks or using a search engine. Part of the goal of this course is to
develop independent thinking in the context of the data science lifecycle,
which can involve a fair bit of exploring and reading documentation. It may be
a bit annoying at first, but you'll get the hang of it, and we're here to guide
you on that path! 

**Hint 3**: To check your answer, `top_names[0]` should return the string
`Peet's Coffee & Tea`. It should not be a number.

<!-- #endregion -->

```{python}
top_names = bus['name'].value_counts().index[:5]
top_addresses = bus['address'].value_counts().index[:5]
# Show the result.
display(top_names)
display(top_addresses)
```

```{python}
top_names[0]
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q1b")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/><br/>

---


## Question 1c

Based on the above exploration, what does each record represent?

**A**. One location of a restaurant.

**B**. A chain of restaurants.

**C**. A city block.

Answer in the following cell. Your answer should be a string, either `"A"`, `"B"`, or `"C"`.

<!-- #endregion -->

```{python}
q1c = "B"
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q1c")
```

<!-- #region {"nbgrader": {"grade": false, "grade_id": "business-data", "locked": true, "schema_version": 2, "solution": false}} -->
<br/><br/><br/>

---

<br/><br/><br/>

# 2: Cleaning the Business Data Postal Codes

The business data contains postal code information that we can use to aggregate the ratings over regions of the city. Let's examine and clean the postal code field. The postal code (sometimes also called a [ZIP code](https://en.wikipedia.org/wiki/ZIP_Code)) partitions the city into regions:

<img src="https://www.usmapguide.com/wp-content/uploads/2019/03/printable-san-francisco-zip-code-map.jpg" alt="ZIP Code Map" style="width: 600px">
<!-- #endregion -->

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/>

---


## Question 2a

How many restaurants are in each ZIP code? 

In the cell below, create a **series** where the index is the postal code and the value is the number of records with that postal code. The series should be in descending order of count. Do you notice any odd/invalid zip codes?

<!-- #endregion -->

```{python nbgrader={'grade': False, 'grade_id': 'cell-d2151d673e6c36a1', 'locked': False, 'schema_version': 2, 'solution': True}}
zip_counts = bus['postal_code'].value_counts()
# Show the result
print(zip_counts.to_string())
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q2a")
```

<!-- #region -->
<br/><br/><br/>

--- 

## Question 2b

In question 2a we noticed a large number of potentially invalid ZIP codes
(e.g., "CA").  These are likely due to data entry errors.  To get a better
understanding of the potential errors in the zip codes, let's break down the
problem into two parts.
<!-- #endregion -->

<!-- #region {"deletable": false, "editable": false} -->
### Part I

Import a list of valid San Francisco ZIP codes by using `pd.read_json` to load
the file `data/sf_zipcodes.json`, and store them as a Series in `valid_zips`.
As you may expect, `pd.read_json` works similarly to `pd.read_csv` but for JSON
files (a different file format we'll cover later) that you can read more about
[here](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html). If
you are unsure of what data type a variable is, remember you can do
`type(some_var_name)` to check!
<!-- #endregion -->

```{python}
valid_zip_numbers = pd.read_json("data/sf_zipcodes.json")
# Show the result
valid_zip_numbers.head()
```

```{python}
valid_zips = valid_zip_numbers['zip_codes'].astype(str)

print(valid_zips[0])
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q2bi")
```

Observe that `pd.read_json` reads data as integers by default. This isn't quite what we want! We would like to store zip codes as strings (you'll learn more about why soon!). To do that, we can use the `astype` function to generate a copy of the Pandas `Series` stored as strings instead.

```{python}
valid_zips = valid_zip_numbers.astype("string")
```

If you're ever unsure about the data type of a variable, remember you can always check using the `type` function like below:

```{python}
type(valid_zips.dtype)
```

Now it's time to do part II. You will probably want to use the `Series.isin` function. For more information on this function see the [the documentation linked in this internet search](https://www.google.com/search?q=series+isin+pandas&rlz=1C1CHBF_enUS910US910&oq=series+isin+pandas&aqs=chrome..69i57l2j69i59j69i60l2j69i65j69i60l2.1252j0j7&sourceid=chrome&ie=UTF-8). 

**Note:** You are welcome and, in fact, encouraged to search and read documentation on the internet to complete the assignments in the course, even if the documentation is not linked explicitly.

<!-- #region {"deletable": false, "editable": false} -->
### Part II

Construct a `DataFrame` containing only the businesses which DO NOT have valid ZIP codes.
<!-- #endregion -->

```{python}
...
invalid_zip_bus = ...
invalid_zip_bus.head(20)
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q2bii")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/><br/>

--- 

## Question 2c

In the previous question, many of the businesses had a common invalid postal code that was likely used to encode a MISSING postal code.  Do they all share a potentially "interesting address"? For that purpose, in the following cells, we will construct a series that counts the number of businesses at each `address` that have this single likely MISSING postal code value.

Let's break this down into steps:

### Part 1
Identify the single most common missing postal code and assign it to
`missing_postal_code`. Then create a Dataframe, `bus_missing`, to store only
those businesses in `bus` that have `missing_postal_code` as their postal code.

**Hint** — remember the `type` of the postal codes.
<!-- #endregion -->

```{python}
missing_postal_code = ...
bus_missing = ...
# Show the result
bus_missing
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q2ci")
```

### Part 2
Using `bus_missing`, find the number of businesses at each address (which would all share the same postal code). Specifically, `missing_zip_address_count` should store a Series with addresses as the indices and the counts as the values.

```{python}
missing_zip_address_count = ...
# Show the result
missing_zip_address_count.head()
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q2cii")
```

<!-- #region {"deletable": false, "editable": false} -->


<br/><br/><br/>

--- 

## Question 2d

If we were to drop businesses with postal code values equal to `missing_postal_code`, what specific types of businesses would we be excluding? In other words, is there a commonality among businesses with missing postal codes?

**Hint**: You may want to identify and Google the names of the businesses with missing postal codes. Feel free to reuse parts of your code from 2c to re-examine `bus_missing`, but we will not be grading your code.

<!-- #endregion -->

<!-- #region {"manual_grade": true, "manual_problem_id": "dropping_zips"} -->
_Type your answer here, replacing this text._
<!-- #endregion -->

<!-- #region {"deletable": false, "editable": false} -->


<br/><br/><br/>

--- 

## Question 2e

Examine the `invalid_zip_bus` Dataframe we computed in question 2c and look at
the businesses that DO NOT have the special MISSING ZIP code value. [Zip
codes](https://en.wikipedia.org/wiki/ZIP_Code) can be just 5 digits, or they
can be the extended ZIP+4 codes, with 5 digits followed by a hyphen and then
four digits, to specify the location more precisely. Some of the invalid postal
codes are just the ZIP+4 codes rather than the first 5 digits. Create a new
column named `postal5` in the original `bus` Dataframe which contains only the
first 5 digits of the `postal_code` column.

Then, for any of the `postal5` ZIP code entries that were not a valid San
Francisco ZIP Code (according to `valid_zips`), the provided code will set the
`postal5` value to `None`. 

**Hint:** You will find `str` accessors particularly useful. They allow you to
use your usual Python string functions in tandem with a Dataframe. For example,
if you wanted to use the `replace` function on every entry in a column of
a Dataframe to change the letter 'a' to 'e', you could do so by writing
`df['col_name'].str.replace('a', 'e')`. Think about the different ways you can
extract the first 5 digits using regular Python code!

**Do not modify the provided code! Simply add your own code in place of the
ellipses.**

<!-- #endregion -->

```{python}
bus['postal5'] = None
...
# Don't modify the code below.
bus.loc[~bus['postal5'].isin(valid_zips), 'postal5'] = None
# Checking the corrected postal5 column
bus.loc[invalid_zip_bus.index, ['bid', 'name', 'postal_code', 'postal5']]
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q2e")
```

<!-- #region {"nbgrader": {"grade": false, "grade_id": "business-data", "locked": true, "schema_version": 2, "solution": false}} -->
<br/><br/><br/>

---

<br/><br/><br/>

# 3: Investigate the Inspection Data

Let's now turn to the inspection DataFrame. Earlier, we found that `ins` has 4 columns named 
`iid`, `score`, `date` and `type`.  In this section, we determine the granularity of `ins` and investigate the kinds of information provided for the inspections. 
<!-- #endregion -->

<!-- #region {"nbgrader": {"grade": false, "grade_id": "cell-174ed23c543ad9da", "locked": true, "schema_version": 2, "solution": false}} -->
Let's start by looking again at the first 5 rows of `ins` to see what we're working with.
<!-- #endregion -->

```{python nbgrader={'grade': False, 'grade_id': 'cell-f0fbe724a2783e33', 'locked': True, 'schema_version': 2, 'solution': False}}
ins.head(5)
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/><br/>

---

## Question 3a

The column `iid` probably corresponds to an inspection id.  Write an expression (line of code) that evaluates to `True` or `False` based on whether all the inspection ids are unique.

**Hint:** This is a very similar question to Question 1a.
<!-- #endregion -->

```{python}
is_ins_iid_unique = ...
# Show the result
is_ins_iid_unique 
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q3a")
```

<!-- #region -->
<br/><br/>

---

## Question 3b

We would like to extract `bid` from each row of the `ins` Dataframe. If we look carefully, the column `iid` of the `ins` DataFrame appears to be the composition of two numbers and the first number looks like a business id.  

Create a new column called `bid` in the `ins` Dataframe containing just the business id.  You will want to use `ins['iid'].str` operations to do this. (Python's in-built `split` method could come in use, read up on the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html)!) Also be sure to convert the type of this column to `int`. 

**Hint**: Similar to an earlier problem where we used `astype("string")` to convert a column to a String, here you should use `astype` to convert the `bid` column into type `int`. **No Python `for` loops or list comprehensions are allowed.**
<!-- #endregion -->

```{python}
ins['bid'] = ...
# Show the first five rows.
ins.head(5)
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q3b")
```

<!-- #region -->
<br/><br/><br/>

---

## Question 3c

For this part, we're going to explore some new somewhat strange syntax that we haven't seen in lecture. Don't panic! If you're not sure what to do, try experimenting, Googling, and don't shy away from talking to other students or course staff.

For this problem we'll use the time component of the inspection data.  All of this information is given in the `date` column of the `ins` Dataframe. 

**No Python `for` loops or list comprehensions are allowed!**
<!-- #endregion -->

<!-- #region {"deletable": false, "editable": false} -->
### Part I

What is the type of the individual `ins['date']` entries? You may want to grab the very first entry and use the `type` function in Python. 
<!-- #endregion -->

```{python}
ins_date_type = ...
# Show the result
ins_date_type
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q3ci")
```

<!-- #region {"deletable": false, "editable": false} -->
### Part II
Rather than the type you discovered in Part 1, we want each entry in `pd.TimeStamp` format. You might expect that the usual way to convert something from it current type to `TimeStamp` would be to use `astype`. You can do that, but the more typical way is to use `pd.to_datetime`. Using `pd.to_datetime`, create a new `ins['timestamp']` column containing `pd.Timestamp` objects.  These will allow us to do date manipulation with much greater ease in part III and part IV. 

Note: You may run into a UserWarning in case you do not specify the date format when using `pd.to_datetime`. To resolve this, consider using the following string '%m/%d/%Y %I:%M:%S %p' to specify the `format`.
<!-- #endregion -->

```{python}
ins['timestamp'] = pd.to_datetime(ins['date'], format= ...
# Show the result
ins['timestamp']
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q3cii")
```

<!-- #region {"deletable": false, "editable": false} -->
### Part III

What are the earliest and latest dates in our inspection data?  

**Hint**: you can use `min` and `max` on dates of the correct type.
<!-- #endregion -->

```{python}
earliest_date = ...
latest_date = ...
# Show the result
print("Earliest Date:", earliest_date)
print("Latest Date:", latest_date)
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q3ciii")
```

<!-- #region {"deletable": false, "editable": false} -->
### Part IV

We probably want to examine the inspections by year. Create an additional `ins['year']` column containing just the year of the inspection.  Consider using `pd.Series.dt.year` to do this.

In case you're curious, the documentation for `TimeStamp` data can be found at [this link](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html#pandas.Timestamp).
<!-- #endregion -->

```{python}
ins['year'] = ...
# Show the first five rows.
ins.head()
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q3civ")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/><br/>

---

<br/><br/><br/>

# 4: Some Analysis

Let's try and figure out whether there are any differences between names of restaurants located in even and odd zipcodes (specifically using the 5-digit postal codes). We will break down this analysis into steps with the end goal of figuring out the restaurant with the longest name and a valid phone number, among all the even zipcodes and odd zipcodes respectively.
<!-- #endregion -->

<!-- #region {"deletable": false, "editable": false} -->
## Question 4a

First, create a new column `name_length` that stores the length of the `name` of each of the restaurants in `bus`. Again, **do not use for loops or list comprehensions**.
<!-- #endregion -->

```{python}
...
# Show the first five rows.
bus.head()
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q4a")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/>

---

## Question 4b

In order to work the 5-digit zipcodes and check whether they are even or odd,
we need to ensure that there are no None values contained. Create a new
Dataframe `bus_valid` which only contains rows with `postal5` values that are
not None. You may find the `.isna()` method useful! For the rest of this
question, we will be working with `bus_valid`.
<!-- #endregion -->

```{python deletable=FALSE, editable=FALSE}
help(pd.Series.isna)
```

```{python}
bus_valid = ...
# Show the result
bus_valid.head()
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q4b")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/>

---

## Question 4c

Now, assign `is_even` to a Boolean **series** that indicates whether the
corresponding 5-digit zipcode in `bus_valid` is even or odd. Remember to keep
in mind the data type of `postal5`!

Hint: You might find the mod operation `%` useful here!
<!-- #endregion -->

```{python}
is_even = ...
# Show the result
is_even
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q4c")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/>

---

## Question 4d 

Using the series you created above, store the name of the business with the longest name amongst all businesses located in even zipcodes in `longest_name_even`.  Make sure that `longest_name_even` contains a **string** with your answer.

**Hint** — feel free to use several code lines for this, but it can be done in
one.
<!-- #endregion -->

```{python}
longest_name_even = ...
# Show the result
longest_name_even
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q4d")
```

<!-- #region {"deletable": false, "editable": false} -->
<br/><br/>

---

## Question 4e

Suppose we only consider businesses in `bus_valid` that also have valid phone
numbers. That is, we no longer include businesses that have invalid phone
numbers. Does the answer you get differ from the answer in 4d, where you
instead restricted yourself to the even zip codes? Here, an invalid phone
number refers to the single common missing value for phone numbers, similar to
the `missing_postal_code` you found in 2b.

Write an expression that indicates whether the longest name is the same (True)
or not (False) if we require that the business must have a valid phone number.
Feel free to use the scratch cell below if need be! You may find your code from
the previous part to be a useful starting point.  Specifically, for this
calculation, you are no longer restricting yourself to even-length names, but
instead to those rows with valid phone numbers.

Don't forget — do not hard-code the answer to True or False - write code that
generates the answer.
<!-- #endregion -->

```{python}
#- SCRATCH CELL
#- Feel free to do your rough work here
#- Do not add a cell between your solution and the grader cell
```

```{python}
same_longest_name = ...
# Show the result
same_longest_name
```

```{python deletable=FALSE, editable=FALSE}
grader.check("q4e")
```

<hr style="border: 5px solid #003262;" />
<hr style="border: 1px solid #fdb515;" />

## Congratulations! You have finished this notebook! ##

Make sure you have run all cells in your notebook in order before running the
cell below, so you can check your answers.



```{python}
grader.check_all()
```
